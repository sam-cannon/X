{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.tree\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sns.set_palette('Set2') #set colorblind-friendly palette for seaborn\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000000000)\n",
    "mpl.rc('figure', max_open_warning = 0)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lag Analysis for Suite NPS\n",
    "- I am going to be analyzing the results of six time lags on Suite NPS\n",
    "- Suite NPS will be regressed on each App's NPS (PP, Excel, Outlook, Word), as well as subtheme's NPS (i.e. 'ease of use')\n",
    "- Each subtheme will be condensed to its root, which will result in a final slice of subthemes accounting for all subtheme tags\n",
    "    - weighted averages will be computed to account for combinations of subthemes\n",
    "    \n",
    "- __TODO: Update the overall analysis with the code for selecting only the single best model every week, code is included in individual app analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data, data since January 2019\n",
    "for file in os.listdir('C:\\\\Users\\\\fulto\\\\Desktop\\\\Lag NPS Analysis'):\n",
    "    \n",
    "    if file.startswith('weekly_app'):\n",
    "        \n",
    "        app = pd.read_csv(file)\n",
    "    \n",
    "    elif file.startswith('weekly_suite'):\n",
    "        \n",
    "        suite = pd.read_csv(file)\n",
    "        \n",
    "    elif file.startswith('weekly_subtheme'):\n",
    "        \n",
    "        subs = pd.read_csv(file).fillna('[\"not mapped\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1447,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the app dataframe\n",
    "\n",
    "#keep only these four apps\n",
    "keep_these_apps = ['Excel', 'PowerPoint', 'Outlook', 'Word']\n",
    "\n",
    "#subset df for these four\n",
    "app = app[app['Feedback_App'].isin(keep_these_apps)]\n",
    "\n",
    "#group df by week and nps\n",
    "app = pd.DataFrame(app.groupby(['Feedback_App', 'Week']).avg_NPS.mean()).reset_index()\n",
    "\n",
    "#create subsets to concatenate - need to reset axes and rename columns for concatenation to work properly\n",
    "excel = app.iloc[:80, :].rename(columns = {'Feedback_App':'feedback_app_excel', 'avg_NPS':'excel_nps'}).drop('Week', axis = 1).reset_index(drop = True)\n",
    "outlook = app.iloc[80:160, :].rename(columns = {'Feedback_App':'feedback_app_outlook', 'avg_NPS':'outlook_nps'}).drop('Week', axis = 1).reset_index(drop = True) \n",
    "pp = app.iloc[160:240, :].rename(columns = {'Feedback_App':'feedback_app_pp', 'avg_NPS':'pp_nps'}).drop('Week', axis = 1).reset_index(drop = True)\n",
    "word = app.iloc[240:, :].rename(columns = {'Feedback_App':'feedback_app_word', 'avg_NPS':'word_nps'}).reset_index(drop = True)\n",
    "\n",
    "#concatenate all these dfs into one\n",
    "app = pd.concat([excel, outlook, pp, word], axis = 1)\n",
    "\n",
    "#move week to the front of the df\n",
    "app.insert(loc = 0, column = 'week', value = [x for x in app['Week']])\n",
    "\n",
    "app.drop('Week', axis = 1, inplace = True)\n",
    "\n",
    "#drop app name columns, just keep nps\n",
    "app = app[['week', 'excel_nps', 'outlook_nps', 'pp_nps', 'word_nps']]\n",
    "\n",
    "#now find the subthemes that stand on their own, that we we can combine the correct ones, this will essentially tell us how many columns we will have\n",
    "themes = pd.DataFrame(subs.groupby(['Verbatim_Themes', 'Week', 'Count']).avg_NPS.mean())\n",
    "\n",
    "themes.reset_index(inplace = True)\n",
    "\n",
    "#there will always be one more element than the number of commas in a list, I only want elements from the column that have no commas\n",
    "unique_subthemes = pd.DataFrame(themes[~themes['Verbatim_Themes'].str.contains(',')].Verbatim_Themes.unique())\n",
    "\n",
    "#lowercase all columns in suite\n",
    "suite.columns = [c.lower() for c in suite.columns]\n",
    "\n",
    "#merge suite and app, clean it up a bit, dropping unnecessary columns\n",
    "df = app.merge(suite, how = 'inner', on = 'week').rename(columns = {'avg_nps':'suite_nps'}).drop(['feedback_platform', 'count'], axis = 1)\n",
    "\n",
    "#replace brackets surrounding elements in list of subtheme names, gotta do this in two stages I think (although I am sure someby knows a better way lol)\n",
    "unique_subthemes = pd.DataFrame(unique_subthemes.iloc[:, 0].str.replace(\"[\",  ''))\n",
    "\n",
    "unique_subthemes = pd.DataFrame(unique_subthemes.iloc[:, 0].str.replace(']',  ''))\n",
    "\n",
    "#remove quotation marks too\n",
    "unique_subthemes = pd.DataFrame(unique_subthemes.iloc[:, 0].str.replace('\"', ''))\n",
    "\n",
    "#make this into a list so we can use the remove operation of lists\n",
    "unique_subthemes = [x for x in unique_subthemes.iloc[:, 0]]\n",
    "\n",
    "\n",
    "#create list of subthemes to remove from this list (we will be adding these into the weighted average) (yammer only had like 11 entries)\n",
    "to_remove = ['easeofuse-howto', 'onedrive for business', 'easeofuse-version', 'not mapped', 'skype for business', 'yammer', 'intune', 'si communication']\n",
    "\n",
    "#remove each element using list.remove()\n",
    "for element in to_remove:\n",
    "    \n",
    "    unique_subthemes.remove(element)\n",
    "    \n",
    "\n",
    "#create dfs for all subthemes\n",
    "def subset_df(theme):\n",
    "    theme_sub = subs[subs['Verbatim_Themes'].str.contains(theme)]\n",
    "    return theme_sub\n",
    "\n",
    "#place each df into a dictionary under its own name, we can subset the dict to access dfs now\n",
    "subdict = {}\n",
    "\n",
    "for theme in unique_subthemes:\n",
    "    \n",
    "    subdict[theme] = pd.DataFrame(subset_df(theme))\n",
    "\n",
    "def build_weighted_avgs(subtheme):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function takes a subtheme as an arg from the list of unique subthemes created previously, it creates a column of weighted average nps for each \n",
    "    week in each subtheme\n",
    "    \n",
    "    args: subtheme - meant to be used in a loop to create several different dfs\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame(subdict[subtheme].groupby(['Verbatim_Themes', 'Week', 'Count']).avg_NPS.mean()).sort_values(['Week', 'Verbatim_Themes']).reset_index()\n",
    "    \n",
    "    all_weeks = [week for week in df['Week'].unique()]\n",
    "    \n",
    "    df['product'] = df['avg_NPS'] * df['Count']\n",
    "    \n",
    "    final_df = pd.DataFrame((pd.DataFrame(df.groupby('Week').product.sum())['product']) / \n",
    "                            (pd.DataFrame(df.groupby('Week').Count.sum())['Count'])).rename(columns = {0:f'{subtheme}_subtheme_nps'})\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "#similar to earlier, create a dict that will hold all of our dfs created with the function above, this will serve as a container for all of our new dfs, which we can then use to \n",
    "# concatenate them all into one df and eventually merge back into the master df\n",
    "weighted_avgs = {}\n",
    "\n",
    "for key, val in subdict.items():\n",
    "    \n",
    "    weighted_avgs[key] = build_weighted_avgs(key)\n",
    "\n",
    "#just to give you an idea of what each one looks like\n",
    "weighted_avg_dfs = []\n",
    "\n",
    "for key, val in weighted_avgs.items():\n",
    "    \n",
    "    weighted_avg_dfs.append(val)\n",
    "    \n",
    "#concatenate them all (merging them together based on the index of Week, the index MUST be \"Week\" or else the df isn't right)\n",
    "subs_weighted_nps = pd.concat(weighted_avg_dfs, axis = 1)\n",
    "\n",
    "#now create final df for regression, concatenating the df we created with app and suite nps with this new subthemes nps df\n",
    "ols_df = pd.concat([df.set_index('week'), subs_weighted_nps], axis = 1)\n",
    "\n",
    "\n",
    "def results_summary_to_dataframe(results):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    take the result of an statsmodel results table and transforms it into a dataframe\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    pvals = results.pvalues\n",
    "    \n",
    "    coeff = results.params\n",
    "    \n",
    "    conf_lower = results.conf_int()[0]\n",
    "    \n",
    "    conf_higher = results.conf_int()[1]\n",
    "\n",
    "    results_df = pd.DataFrame({\"pvals\":pvals,\n",
    "                               \"coeff\":coeff,\n",
    "                               \"conf_lower\":conf_lower,\n",
    "                               \"conf_higher\":conf_higher\n",
    "                                })\n",
    "\n",
    "    #Reordering...\n",
    "    results_df = results_df[[\"coeff\",\"pvals\",\"conf_lower\",\"conf_higher\"]]\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "#function for simply lagging the df\n",
    "def lag_df(df, lags):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function lags our df by every column except for suite nps\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    lagged_df = pd.concat([df.loc[:, df.columns != 'suite_nps'].shift(lags), df['suite_nps']], axis = 1)\n",
    "    \n",
    "    return lagged_df\n",
    "\n",
    "#just add a constant in right here for regression\n",
    "ols_df['constant'] = 1\n",
    "\n",
    "results_dfs = []\n",
    "\n",
    "#create list of lags to iterate through\n",
    "lags = list(range(0, 7, 1))\n",
    "\n",
    "#create list of row ids to iterate through\n",
    "rows = list(range(0, 80, 1))\n",
    "\n",
    "#now forming a nested loop that will iterate first through the lagged df rows, then move on to running a regression for each row in that lagged df\n",
    "# since regressions will not be able to be run for certain lag periods for certain rows, we are bypassing errors \n",
    "for row in rows:\n",
    "    \n",
    "    #iterate through list of lags\n",
    "    for lag in lags:\n",
    "    \n",
    "        #try to execute the below code, if an error is encountered then pass it by\n",
    "        try:\n",
    "            \n",
    "            #lag the df in steps, iterating through the list of lags (so 1 through 6)\n",
    "            reg_df = lag_df(ols_df, lag)\n",
    "        \n",
    "            #create a df that is just one row of the ols_df to subset for regression\n",
    "            regression_row = pd.DataFrame(reg_df.iloc[row, :]).T.dropna(axis = 1)\n",
    "            \n",
    "            #set the target to suite nps\n",
    "            target = regression_row['suite_nps']\n",
    "    \n",
    "            #set features to every other column in that row\n",
    "            features = regression_row.drop('suite_nps', axis = 1)\n",
    "        \n",
    "            #instantiate the model\n",
    "            model = sm.OLS(target, features)\n",
    "        \n",
    "            #run the model and get results set\n",
    "            results = model.fit()\n",
    "        \n",
    "            #send the results set to a dataframe\n",
    "            ols_results = results_summary_to_dataframe(results)\n",
    "            \n",
    "            #create a column with the AIC (goodness of fit metric to compare to other models -- as AIC decreases, model gets better)\n",
    "            ols_results['AIC'] = results.aic\n",
    "            \n",
    "            #create column for model's BIC, same as AIC for all intents and purposes\n",
    "            ols_results['BIC'] = results.bic\n",
    "            \n",
    "            #create a tag to denote which lag the results set is from\n",
    "            ols_results['lag_tag'] = lag\n",
    "            \n",
    "            #create tag to indicate which row out of the ols_df the regression was run on\n",
    "            ols_results['row_tag'] = row\n",
    "            \n",
    "            #create week column that denotes the week of the data that the regression was run on -- this will be the week of the suite_nps value\n",
    "            ols_results['week'] = ols_df.reset_index().iloc[row, 0]\n",
    "            \n",
    "            #append these results dfs to the list outside of the loop so that we can access them\n",
    "            results_dfs.append(ols_results)\n",
    "            \n",
    "        #pass by errors    \n",
    "        except:\n",
    "        \n",
    "            pass\n",
    "\n",
    "def create_results_row(results_dfs, index):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function takes a results set df generated by the regression loop above and then makes the results fit onto one row, the goal\n",
    "    here is to concatenate all of the rows eventually to have one whole results df containing all of the results for each lag and row\n",
    "    \n",
    "    args: results_dfs is the list of results dfs generated in the previous loop, index is the subset number, so this is the number of each individual\n",
    "    df from the results dfs list, this function is meant to be looped over so the index is each number from 1-459\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #transpose the df \n",
    "    df = results_dfs[index].T\n",
    "    \n",
    "    #take this value and set it as a column, do the same for the next 4 things too\n",
    "    df['aic'] = df.iloc[4, 0]\n",
    "    \n",
    "    df['bic'] = df.iloc[5, 0]\n",
    "    \n",
    "    df['lag'] = df.iloc[6, 0]\n",
    "    \n",
    "    df['suite_nps_row_num'] = df.iloc[7, 0]\n",
    "    \n",
    "    df['suite_nps_week'] = df.iloc[8, 0]\n",
    "    \n",
    "    #take the first row of the df that we created, transpose this as well to turn it into a single row df\n",
    "    df = pd.DataFrame(df.iloc[0, :]).T\n",
    "    \n",
    "    return df\n",
    "\n",
    "#here we are just running a loop that will go through every single index number of the results dfs (0 - 458 here) and use the function we created above\n",
    "# to extract the results as a single row that we can use to concatenate to other rows like it to form a final results df\n",
    "\n",
    "#create empty list to store rows we are creating\n",
    "final_results = []\n",
    "\n",
    "#create list of index values to use in subsetting the results df (0-458)\n",
    "df_num = list(range(0, 539, 1))\n",
    "\n",
    "#looping through our indices and appending the new rows to our list\n",
    "for num in df_num:\n",
    "    \n",
    "    results_row = create_results_row(results_dfs, num)\n",
    "    \n",
    "    final_results.append(results_row)\n",
    "    \n",
    "#concatenating all of our new rows into one final df\n",
    "final_results = pd.concat(final_results).reset_index(drop = True)\n",
    "\n",
    "#format the final df how I want it, with week first\n",
    "first_cols = final_results.iloc[:, 37:42]\n",
    "first_cols = first_cols[first_cols.columns[::-1]]\n",
    "\n",
    "#need to re-order columns\n",
    "final_results = pd.concat([first_cols, final_results.drop(final_results.columns[37:42], axis = 1)], axis = 1)\n",
    "\n",
    "#merge back in suite numbers\n",
    "final_results = pd.merge(suite, final_results, how='left', left_on=['week'], right_on=['suite_nps_week']).sort_values(['suite_nps_row_num', 'lag'])\\\n",
    "            .rename(columns = {'avg_nps':'suite_nps', 'count':'suite_count', 'suite_nps_row_num':'week_block'}).drop(['suite_nps_week', 'feedback_platform'], axis = 1)\n",
    "\n",
    "#make week datetime so it transfers to the csv correctly\n",
    "final_results['week'] = pd.to_datetime(final_results['week'])\n",
    "\n",
    "#send to csv\n",
    "#final_results.to_csv('weekly_suite_lag_regression_analysis.csv', index = False, date_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potentially Using AIC and BIC Together With Coefficients to Determine Best Models\n",
    "- I think that using the AIC and BIC may one (if not the only) way we can determine whether one model is better than another in this scenario/methodology\n",
    "- Since we are running a regression on such little data and with only one single DV value, traditional statistics will not be able to be calculated properly (i.e. R^2). Due to this reality, I think it makes sense to use these fit criteria when they show a clear frontrunner(s). If AIC and BIC are the same for two models, then perhaps looking into the coefficients of apps as differentiators may be prudent. A summary of AIC and BIC is here:\n",
    "\n",
    "AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.\n",
    "\n",
    "AIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.\n",
    "\n",
    "So what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Same Analysis For Individual Apps\n",
    "- Now, instead of running a regression on all of the columns, I will be running it only on single apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1448,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepping dfs, sorting values so that I can concatenate them\n",
    "suite.sort_values('week', inplace = True)\n",
    "\n",
    "app.sort_values('week', inplace = True)\n",
    "\n",
    "app_lag = pd.concat([app, suite[['avg_nps', 'count']]], axis = 1)\n",
    "\n",
    "app_lag.rename(columns = {'avg_nps':'suite_nps', 'count':'suite_count'}, inplace = True)\n",
    "\n",
    "#modifying this function to pull only coefficients\n",
    "def results_summary_to_dataframe(results):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    take the result of an statsmodel results table and transforms it into a dataframe\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    coeff = results.params\n",
    "    \n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "                               \"coeff\":coeff\n",
    "                                })\n",
    "\n",
    "    #Reordering...\n",
    "    results_df = results_df[[\"coeff\"]]\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "#setting up empty list to snatch up results dfs\n",
    "app_summaries = []\n",
    "\n",
    "#create list of lags to iterate through\n",
    "lags = list(range(0, 7, 1))\n",
    "\n",
    "#create list of row ids to iterate through\n",
    "rows = list(range(0, 80, 1))\n",
    "\n",
    "columns = ['excel_nps', 'outlook_nps', 'pp_nps', 'word_nps']\n",
    "\n",
    "#now forming a nested loop that will iterate first through the lagged df rows, then move on to running a regression for each row in that lagged df\n",
    "# since regressions will not be able to be run for certain lag periods for certain rows, we are bypassing errors \n",
    "for row in rows:\n",
    "    \n",
    "    for column in columns:\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            #iterate through list of lags\n",
    "            for lag in lags:\n",
    "    \n",
    "                #try to execute the below code, if an error is encountered then pass it by\n",
    "            \n",
    "                #lag the df in steps, iterating through the list of lags (so 1 through 6)\n",
    "                reg_df = lag_df(app_lag, lag)\n",
    "        \n",
    "                #create a df that is just one row of the ols_df to subset for regression\n",
    "                regression_row = pd.DataFrame(reg_df.iloc[row, :]).T.reset_index(drop = True)\n",
    "            \n",
    "                #set the target to suite nps\n",
    "                target = regression_row['suite_nps']\n",
    "    \n",
    "                #set features to every other column in that row\n",
    "                features = regression_row[column]\n",
    "                features = pd.DataFrame(features).T.rename(columns = {0:column}).reset_index(drop = True)\n",
    "            \n",
    "                #add in constant\n",
    "                features['constant'] = 1\n",
    "        \n",
    "                #instantiate the model\n",
    "                app_model = sm.OLS(target.astype(float), features.astype(float))\n",
    "        \n",
    "                #run the model and get results set\n",
    "                results = app_model.fit()\n",
    "                \n",
    "                app_ols_results = results_summary_to_dataframe(results)\n",
    "            \n",
    "                #create a column with the AIC (goodness of fit metric to compare to other models -- as AIC decreases, model gets better)\n",
    "                app_ols_results['AIC'] = results.aic\n",
    "            \n",
    "                #create column for model's BIC, same as AIC for all intents and purposes\n",
    "                app_ols_results['BIC'] = results.bic\n",
    "            \n",
    "                #create a tag to denote which lag the results set is from\n",
    "                app_ols_results['lag'] = lag\n",
    "            \n",
    "                #create week column that denotes the week of the data that the regression was run on -- this will be the week of the suite_nps value\n",
    "                app_ols_results['week_block'] = app_lag.reset_index().iloc[row, 0] + 1\n",
    "            \n",
    "                #append these results dfs to the list outside of the loop so that we can access them\n",
    "                app_summaries.append(app_ols_results)\n",
    "            \n",
    "            #pass by errors    \n",
    "        except:\n",
    "        \n",
    "            pass\n",
    "\n",
    "#concqtenate all of the dfs in our list\n",
    "summs = pd.concat(app_summaries)\n",
    "\n",
    "#subset the dataframe, removing the constant coefficients\n",
    "summs = summs[summs.index != 'constant'].reset_index().rename(columns = {'index':'app'})\n",
    "\n",
    "#create a df with unique weeks and suite nps scores that we can merge back into our results df\n",
    "week_df = pd.concat([pd.DataFrame(app_lag.week.unique()).rename(columns = {0:'week'}), app_lag[['suite_nps', 'suite_count']]], axis = 1)\n",
    "\n",
    "#create a week block in this df to use as a primary key in our merge\n",
    "week_df['week_block'] = [x for x in list(range(1, 81, 1))]\n",
    "\n",
    "#merge the two\n",
    "summs = week_df.merge(summs, on = 'week_block')\n",
    "\n",
    "#convert week to datetime so format is kept when sending to csv\n",
    "summs['week'] = pd.to_datetime(summs['week'])\n",
    "\n",
    "#keep only the best fitting model for each week, if there is a tie between AIC and BIC, just keep the model with the least amount of lags\n",
    "apps = ['excel_nps', 'outlook_nps', 'pp_nps', 'word_nps']\n",
    "\n",
    "#create empty dict to store dfs\n",
    "app_final_results = {}\n",
    "\n",
    "#loop through apps and create final results dfs\n",
    "for app in apps:\n",
    "    \n",
    "    app_final_results[app] = pd.DataFrame(summs[(summs['app'] == app) & (~summs['AIC'].astype(str).str.contains('-inf'))]\\\n",
    "             .groupby(['week', 'suite_nps', 'suite_count', 'week_block', 'coeff', 'lag', 'AIC']).BIC.min()).reset_index().sort_values(['week', 'lag'])\\\n",
    "                    .drop_duplicates(subset = ['week', 'AIC', 'BIC'], keep = 'first')\\\n",
    "                        .sort_values(['week', 'AIC']).drop_duplicates(subset = ['week'], keep = 'last').reset_index(drop = True)\n",
    "\n",
    "#create 4 separate dfs for final results outputs, send to csvs\n",
    "app_final_results['excel_nps'].to_csv('suite_lag_regressed_on_excel.csv', index = False, date_format = '%Y-%m-%d')\n",
    "app_final_results['outlook_nps'].to_csv('suite_lag_regressed_on_outlook.csv', index = False, date_format = '%Y-%m-%d')\n",
    "app_final_results['pp_nps'].to_csv('suite_lag_regressed_on_pp.csv', index = False, date_format = '%Y-%m-%d')\n",
    "app_final_results['word_nps'].to_csv('suite_lag_regressed_on_word.csv', index = False, date_format = '%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
