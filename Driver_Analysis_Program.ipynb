{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.tree\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "sns.set_palette('Set2') #set colorblind-friendly palette for seaborn\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000000000)\n",
    "mpl.rc('figure', max_open_warning = 0)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Analysis Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class essentially houses all of the functions necessary to run a driver analysis\n",
    "class driver_analysis:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This class contains a collection of all functions written for conducting a driver analysis for any app or all of suite\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \n",
    "        df = df\n",
    "        \n",
    "    def discrete_nums(df):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        bins numeric features in dfs, this is a compilation of all of the hard work involved in individually binning these feature by hand once, I want\n",
    "        to make sure that I NEVER have to do it this way again, the arg is just a df, and the output is that same df with binned features if they are included in\n",
    "        this list\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        for feature in df.columns:\n",
    "            \n",
    "            if feature == 'Device_SysVolSizeMB':\n",
    "        \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 30000, 100000, 500000, 1000000, 1000000000000000], \n",
    "                  labels= ['< 30GB', '30GB-100GB', '100GB-500GB', '500GB-1,000GB', '1,000GB+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_ExchangeSubscriptionsCount':\n",
    "                \n",
    "                df[feature] = (pd.cut(df[feature],  bins = [0, 4, 5, 6, 8, 9, 10, 149],\n",
    "                                       labels = ['< 4', '4-5', '5-6', '6-8', '8-9', '9-10', '10+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Subscription_Count':\n",
    "                \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 2, 4, 5, 8, 11, 15, 19, 332],\n",
    "                                       labels = ['< 2', '2-4', '4-5', '5-8', '8-11', '11-15', '15-19', '19-332'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Subscription_Sum_OfferEDU':\n",
    "                \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 2, 3, 5, 7, 9, 269],\n",
    "                                       labels = ['< 2', '2-3', '3-5', '5-7', '7-9', '9+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Device_BrowserVersion':\n",
    "                \n",
    "                df['Device_BrowserVersion'] = df['Device_BrowserVersion'].astype(str)\n",
    "                \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 76, 80, 81, 85],\n",
    "                                       labels = ['< 76', '76-80', '80-81', '81-85'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_TrialAvailableUnits':\n",
    "                \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 25, 50, 10000, 500000, 1000000, 2000000, 3000000],\n",
    "                                       labels = ['< 25', '25-50', '50-10,000', '10,000-500,000', '500,000-1,000,000', '1-2 million', '2-3 million'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_OfficeAvailableUnits':\n",
    "                \n",
    "                df[feature] = (pd.cut(df[feature], bins= [0, 2000, 12000, 60000, 300000, 600000, 750000, 1500000, 100000000000000],\n",
    "                    labels =['< 2,000', '2,000-12,000', '12,000-60,000', '60,000-300,000', '300,000-600,000', '600,000-750,000', '750,000-1,500,000', '1,500,000+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_TotalSubscriptionsCount':\n",
    "                \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 7, 12, 18, 23, 29, 1000],\n",
    "                                       labels = ['< 7', '7-12', '12-18', '18-23', '23-29', '29+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'User_Commercial_TotalAvailableUnits':\n",
    "                \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 100000, 500000, 750000, 1000000, 1500000, 2000000, 3000000],\n",
    "                                       labels = ['< 100,000', '100,000-500,000', '500,000-750,000', '750,000-1,000,000', \n",
    "                                                 '1,000,000-1,500,000', '1,500,000-2,000,000', '2,000,000-3,000,000'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_ProPlusAvailableUnits':\n",
    "                \n",
    "                df[feature] = (pd.cut(df[feature], bins= [0, 100, 1000, 5000, 20000, 50000, 150000, 300000, 100000000000],\n",
    "                                       labels = ['< 100', '100-1,000', '1,000-5,000', '5,000-20,000', '20,000-50,000', '50,000-150,000', '150,000-300,000', '300,000+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Device_SysVolFreeSpaceMB':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 30000, 60000, 90000, 170000, 200000, 350000, 500000, 750000, 10000000000000], \n",
    "                              labels = ['< 30GB', '30GB-60GB', '60GB-90GB', '90GB-170GB', '170GB-200GB', '200GB-350GB', '350GB-500GB', '500GB-750GB', '750GB+'])).astype(str)\n",
    "        \n",
    "            elif feature == 'Device_ProcSpeedMHz':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 1500, 2000, 2500, 3000, 3500, 4000, 4500], labels = ['< 1500', '1,500-2,000', '2,000-2,500', '2,500-3,000', '3,000-3,500',\n",
    "                                                                                    '3,500-4,000', '4,000-4,500'])).astype(str)\n",
    "        \n",
    "            elif feature == 'User_Consumer_TenureDays':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 100, 250, 365, 730, 1095, 1460, 1825, 2190, 2571], labels = ['< 3 Months', '3-9 Months', \n",
    "                    '9 Months-1 Year', '1-2 Years', '2-3 Years', '3-4 Years', '4-5 Years', '5-6 Years', '6+ Years'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Device_HorizontalResolution':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 1400, 1800, 2000, 2500, 5000, 10000],\n",
    "                      labels = ['< 1,400', '1,400-1,800', '1,800-2,000', '2,000-2,500', '2,500-5,000', '5,000+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Device_RamMB':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 4000, 8000, 16000, 50000, 100000],\n",
    "                              labels = ['< 40GB', '40GB-80GB', '80GB-160GB', '160GB-500GB', '500GB-1,000GB'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_Odin_TotalUsers':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 5, 50, 150, 750, 5000, 15000, 40000, 100000, 200000, 10000000000000],\n",
    "                          labels = ['< 5', '5-50', '50-150', '150-750', '750-5,000', '5-15,000', '15-40,000', '40-100,000', '100-200,000', '200,000+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_Tenure':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 30, 90, 365, 730, 1095, 1460, 1825, 2190, 2555, 2920],\n",
    "                          labels = ['< 1 month', '1-3 months', '3 months-1 year', '1-2 years', '2-3 years', '3-4 years', '4-5 years', '5-6 years', '6-7 years', '7-8 years'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_PaidAvailableUnits':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 5, 30, 100, 300, 2500, 10000, 30000, 200000, 10000000000000000],\n",
    "                  labels = ['< 5', '5-30', '30-100', '100-300', '300-2,500', '1,500-10,000', '10-30,000', '30-200,000', '200,000+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_EXOEnabledUsers':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 10, 50, 200, 1000, 5000, 10000, 50000, 150000, 10000000000000],\n",
    "                      labels = ['< 10', '10-50', '50-200', '200-1,000', '1-5,000', '5-10,000', '10-50,000', '50-150,000', '150,000+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_LYOEnabledUsers':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 12, 55, 400, 3000, 15000, 70000, 1000000000000],\n",
    "                      labels = ['< 12', '12-55', '55-400', '400-3,000','3-15,000', '15-70,000', '70,000+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'User_Commercial_TotalEnabledUsers':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 20, 100, 300, 2000, 10000, 30000, 80000, 200000, 1000000000000],\n",
    "                      labels = ['< 20', '20-100', '100-300', '300-2,000', '2-10,000', '10-30,000', '30-80,000', '80-200,000', '200,000+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_TotalEnabledUsers':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 20, 100, 250, 1000, 5000, 20000, 50000, 100000, 250000, 100000000000000],\n",
    "                      labels = ['< 20', '20-100', '100-250', '250-1,000', '1-5,000', '5-20,000', '20-50,000', '50-100,000', '100-250,000', '250,000+'])).astype(str)\n",
    "            \n",
    "            elif feature == 'Tenant_SPOEnabledUsers':\n",
    "            \n",
    "                df[feature] = (pd.cut(df[feature], bins = [0, 10, 100, 1000, 5000, 10000, 15000, 30000, 75000, 150000, 1000000000],\n",
    "                  labels = ['< 10', '10-100', '100-1,000', '1-5,000', '5-10,000', '10-15,000', '15-30,000', '30-75,000', '75-150,000', '150,000+'])).astype(str)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    #remove columns from data that are missing all or almost all of their data (in this case more than half)\n",
    "    def remove_completely_missing_columns(df, missing_tolerance):\n",
    "    \n",
    "        '''this function removes all columns that are missing more than x% of all values'''\n",
    "    \n",
    "    \n",
    "        #create missingness df\n",
    "        missingness = pd.DataFrame(df.isna().sum()).reset_index().rename(columns = {'index':'column', 0:'num_missing'})\n",
    "\n",
    "        print(f'shape before deleting missing columns: {df.shape}')\n",
    "        #find columns that are missing all of their data and drop them\n",
    "        all_missing = []\n",
    "        for idx, row in missingness.iterrows():\n",
    "            #filter out columns missing most of their values\n",
    "            if row[1] > (missing_tolerance * df.shape[0]):\n",
    "                all_missing.append(row[0])\n",
    "\n",
    "        #drop these columns\n",
    "        df = df.drop([x for x in all_missing], axis = 1)\n",
    "\n",
    "        print(f'shape after deleting missing columns: {df.shape}')\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def target_encoder(df, column, target, index=None, method='mean'):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Target-based encoding is numerization of a categorical variables via the target variable. Main purpose is to deal\n",
    "        with high cardinality categorical features without exploding dimensionality. This replaces the categorical variable\n",
    "        with just one new numerical variable. Each category or level of the categorical variable is represented by a\n",
    "        summary statistic of the target for that level.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            df (pandas df): Pandas DataFrame containing the categorical column and target.\n",
    "            column (str): Categorical variable column to be encoded.\n",
    "            target (str): Target on which to encode.\n",
    "            index (arr): Can be supplied to use targets only from the train index. Avoids data leakage from the test fold\n",
    "            method (str): Summary statistic of the target. Mean, median or std. deviation.\n",
    "            \n",
    "        Returns:\n",
    "\n",
    "            arr: Encoded categorical column.\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        index = df.index if index is None else index # Encode the entire input df if no specific indices is supplied\n",
    "\n",
    "\n",
    "\n",
    "        if method == 'mean':\n",
    "\n",
    "            encoded_column = df[column].map(df.loc[index].groupby(column)[target].mean())\n",
    "\n",
    "        elif method == 'median':\n",
    "\n",
    "            encoded_column = df[column].map(df.loc[index].groupby(column)[target].median())\n",
    "\n",
    "        elif method == 'std':\n",
    "\n",
    "            encoded_column = df[column].map(df.loc[index].groupby(column)[target].std())\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError(\"Incorrect method supplied: '{}'. Must be one of 'mean', 'median', 'std'\".format(method))\n",
    "\n",
    "\n",
    "\n",
    "        return encoded_column\n",
    "    \n",
    "    #updated this for app from suite, just had to add in a couple of columns that weren't in this originally\n",
    "    def throw_out_useless_features(df):\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        There are several features that Manoj doesn't want in the analysis (i.e. locators since every geo feature rolls up to region), so I am creating a function\n",
    "        that takes care of this since I have to do it so often\n",
    "        \n",
    "        IMPORTANT: Tenant_Region is usually removed in favor of User_A13Region, which is a higher-order regional variable, but with Teams we keep Tenant_Region becuase \n",
    "        there is no User_A13Region in the dataset. The same applies to Tenant_Tenure, this seems to be a big driver in Teams and should not be sacrificed in place of \n",
    "        User_Consumer_TenureDays\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        print(f'Shape before: {df.shape}')\n",
    "        \n",
    "        #get rid of features that we know from Surface analysis that Manoj definitely does not want in the model\n",
    "        features = pd.DataFrame(df.columns).rename(columns = {0:'features'})\n",
    "\n",
    "        features = features[~features['features']\\\n",
    "        .str.contains('Feedback_ID|Date|Feedback_Verbatim|Feedback_SurveyName|Scale|Locale|Feedback_SurveyFlightName|'\\\n",
    "                  'IsValid|Language|SourceVerbatim|Ocv|Country|State|OsName|User_SurveyId|Enabled|Device_SysVolSizeMB')]\n",
    "\n",
    "        features = features[~features['features']\\\n",
    "        .str.contains('User_CommercialTenantName|MSSalesTopParent|Market|ResellerName|Tenant_Name|City|'\\\n",
    "                  'OrgName|OrgId|Unnamed|TenantName|TenantID|OfferList|Time|LastUpdated|Feedback_Platform|Tenant_CountryCode')]\n",
    "\n",
    "        features = features[~features['features']\\\n",
    "        .str.contains('Postal|Currency|SubsidiaryName|SubRegion|Cluster|RegionName|AreaName|Big|VerticalName|'\\\n",
    "                  'SegmentName|GroupName|ATUName|Territory|TenantId|Month|Rating_y|App_y|AppModeExt|Host|Tenant_City|Tenant_Country')]\n",
    "\n",
    "        features = features[~features['features']\\\n",
    "        .str.contains('CompanyName|GPName|SignupRegion|AreaName|TeamName|Verbatim_Details|Verbatim_Themes|Tenant_Region|NPS|'\\\n",
    "                  'Fork|NgramThemePrediction|DistrictId|Since|Start|End|Ngrams|Rating_x|PremierSchedule|SubsidiaryId')]\n",
    "\n",
    "        #subset the df \n",
    "        df = df[[x for x in features['features']]]\n",
    "    \n",
    "        print(f'Shape after throwing out useless features: {df.shape}')\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def add_value_labels(ax, labels, df, spacing=5):\n",
    "        \n",
    "        \"\"\"Add labels to the end of each bar in a bar chart.\n",
    "\n",
    "            Arguments:\n",
    "            ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n",
    "            of the plot to annotate.\n",
    "            spacing (int): The distance between the labels and the bars.\n",
    "        \"\"\"\n",
    "    \n",
    "        i = 0\n",
    "    \n",
    "        # For each bar: Place a label\n",
    "        for rect in ax.patches:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            y_value = rect.get_height()\n",
    "            x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "            # Number of points between bar and label. Change to your liking.\n",
    "            space = spacing\n",
    "            # Vertical alignment for positive values\n",
    "            va = 'bottom'\n",
    "\n",
    "            # If value of bar is negative: Place label below bar\n",
    "            if y_value < 0:\n",
    "                # Invert space to place label below\n",
    "                space *= -1\n",
    "                # Vertically align label at top\n",
    "                va = 'top'\n",
    "\n",
    "            # Use Y value as label and format number with one decimal place\n",
    "            label = f\"{labels[i]}\"\n",
    "        \n",
    "\n",
    "\n",
    "            # Create annotation\n",
    "            ax.annotate(\n",
    "                    label,                      # Use `label` as label\n",
    "                    (x_value, y_value),         # Place label at end of the bar\n",
    "                    xytext=(0, space),          # Vertically shift label by `space`\n",
    "                    textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "                    ha='center',                # Horizontally center label\n",
    "                    va=va) # Vertically align label differently for\n",
    "                                        # positive and negative values.\n",
    "        \n",
    "            i += 1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def plot_cats(df, subset = None):\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        This function takes a df as an argument, it plots categorical features, if a categorical feature has more than 40 levels then it simply\n",
    "        performs a groupby and chooses the top 20 from that feature's value counts, indications of this will be in the title of the resulting plot\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        #loop through and plot each cat feature with less than 40 levels\n",
    "        for feature in df.columns:\n",
    "        \n",
    "            if (df[feature].nunique() < 40) & (feature != 'Feedback_RatingValue') & (df[feature].dtype == 'O'): # don't want to plot feedback rating value\n",
    "        \n",
    "                plt.figure(figsize = (20, 7))\n",
    "        \n",
    "                ax = df.groupby(df[feature]).Feedback_RatingValue.mean().sort_values().plot(kind = 'bar')\n",
    "        \n",
    "                plt.title(f'NPS by {feature}', fontsize = 16)\n",
    "        \n",
    "                plt.xlabel(f'{feature}', fontsize = 14)\n",
    "        \n",
    "                plt.ylabel('Average NPS Score', fontsize = 14)\n",
    "            \n",
    "                #labels = [int(x) for x in pd.DataFrame(df[feature].value_counts()).reset_index(drop = False).iloc[:,1]]\n",
    "                \n",
    "                labels = [x for x in df.groupby(f'{feature}').Feedback_RatingValue.agg(['mean', 'size']).sort_values('mean')['size']]\n",
    "                \n",
    "                driver_analysis.add_value_labels(ax, labels, df)\n",
    "\n",
    "            \n",
    "                #plt.axhline(y=nps, color = 'tab:blue', linestyle = '--', linewidth = 2)#create lineplot for NPS trend overall\n",
    "            \n",
    "                #if a feature has more than 20 levels, rotate this much    \n",
    "                if df[feature].nunique() > 20:\n",
    "            \n",
    "                    plt.xticks(rotation = 80, fontsize = 12)\n",
    "                \n",
    "                else:\n",
    "            \n",
    "                    plt.xticks(rotation = 40, fontsize = 12)\n",
    "                \n",
    "            \n",
    "#                 #add annotated legend depending on if data is suite or app\n",
    "#                 if subset == 'web':\n",
    "                \n",
    "#                     plt.legend(labels = ['2020 AVG Web NPS'], fontsize = 14)\n",
    "            \n",
    "#                 elif subset == 'windows':\n",
    "                \n",
    "#                     plt.legend(labels = ['2020 AVG Windows NPS'], fontsize = 14)\n",
    "                \n",
    "            elif (df[feature].nunique() > 40) & (df[feature].dtype == 'O'):\n",
    "            \n",
    "                #plotting top 20 device models by NPS score \n",
    "                top_20 = pd.DataFrame(df[f'{feature}'].value_counts().nlargest(20)).index\n",
    "            \n",
    "                #creating df for levels in the top 20\n",
    "                plt_df = df[df[f'{feature}'].isin(top_20)]\n",
    "            \n",
    "                #creating figure size\n",
    "                plt.figure(figsize = (20, 7))\n",
    "            \n",
    "                #plotting on a categorical variable using groupby\n",
    "                ax = plt_df.groupby(f'{feature}').Feedback_RatingValue.mean().sort_values().plot(kind = 'bar')\n",
    "            \n",
    "                #create title\n",
    "                plt.title(f'NPS by Top 20 {feature}', fontsize = 16)\n",
    "            \n",
    "                #create y label\n",
    "                plt.ylabel('Average NPS Score', fontsize = 14)\n",
    "            \n",
    "                #create x label\n",
    "                plt.xlabel(f'{feature}', fontsize = 14)\n",
    "                \n",
    "                #labels = [int(x) for x in pd.DataFrame(df[feature].value_counts().nlargest(20)).reset_index(drop = False).iloc[:,1]]\n",
    "                \n",
    "                labels = [x for x in plt_df.groupby(f'{feature}').Feedback_RatingValue.agg(['mean', 'size']).sort_values('mean')['size']]\n",
    "                \n",
    "                driver_analysis.add_value_labels(ax, labels, df)\n",
    "\n",
    "            \n",
    "                #plt.axhline(y=nps, color = 'tab:blue', linestyle = '--', linewidth = 2)#create lineplot for NPS trend overall\n",
    "            \n",
    "#                 if subset == 'web':\n",
    "            \n",
    "#                     plt.legend(labels = ['2020 AVG Web NPS'], fontsize = 14)\n",
    "                \n",
    "#                 elif subset == 'windows':\n",
    "                \n",
    "#                     plt.legend(labels = ['2020 AVG Windows NPS'], fontsize = 14)\n",
    "                \n",
    "                if df[feature].nunique() > 20:\n",
    "            \n",
    "                    plt.xticks(rotation = 80, fontsize = 12)\n",
    "                \n",
    "                else:\n",
    "            \n",
    "                    plt.xticks(rotation = 80, fontsize = 12);\n",
    "                #setting x tick rotation for longer names to be visible and not overlap\n",
    "                \n",
    "    def plot_office_build_by_age(df, nps):\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        This function plots office build sorted by age\n",
    "    \n",
    "        '''\n",
    "\n",
    "        a = pd.DataFrame(df['Device_OfficeBuild'].value_counts().nlargest(20)).reset_index().sort_values('index').rename(columns = {'index':'OfficeBuild', 'Device_OfficeBuild':\n",
    "                                                                                                                                      'number'})\n",
    "\n",
    "        b = pd.DataFrame(df.groupby('Device_OfficeBuild').Feedback_RatingValue.mean()).reset_index()\n",
    "\n",
    "        b = b[b['Device_OfficeBuild'].isin(a['OfficeBuild'])].sort_values('Device_OfficeBuild', ascending = False)\n",
    "\n",
    "        pd.DataFrame(b.groupby('Device_OfficeBuild').Feedback_RatingValue.mean()).sort_values(by = 'Device_OfficeBuild', ascending = False).plot(kind = 'bar', figsize = (15, 8))\n",
    "        plt.xlabel('Office Build Sorted by Age', fontsize = 16)\n",
    "        plt.ylabel('Average NPS', fontsize = 16)\n",
    "        plt.title('NPS by Office Build (Sorted by Age)', fontsize = 22)\n",
    "        plt.axhline(nps, linestyle = '--', color = 'tab:blue')\n",
    "        plt.legend(labels = ['2020 AVG NPS'], fontsize = 14)\n",
    "        plt.xticks(rotation = 45, fontsize = 12)\n",
    "            \n",
    "    def numeric_plot_structure(df, feature, title, xlabel, bins, labels = None, method = None):\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        This function takes a df, feature, binsize (or individual bins when using cut), labels (again, if using cut pass a list of labels), and method\n",
    "        It then plots the feature accordingly\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        #if you are going to use qcut\n",
    "        if method is 'qcut':\n",
    "            \n",
    "            plt.figure(figsize = (15, 7))\n",
    "                \n",
    "            ax = df.groupby(pd.qcut(df[feature], bins, duplicates = 'drop')).Feedback_RatingValue.mean().plot(kind = 'bar')\n",
    "                \n",
    "            plt.title(title, fontsize = 16)\n",
    "                \n",
    "            plt.xlabel(xlabel, fontsize = 14)\n",
    "                \n",
    "            plt.ylabel('Average NPS Score')\n",
    "            \n",
    "            plt.xticks(list(range(0, bins)), labels, rotation = 50, fontsize = 12);\n",
    "        \n",
    "        #if you are going to use cut (for descretizing features with greater control over bins)\n",
    "        elif method is 'cut':\n",
    "            \n",
    "            plt.figure(figsize = (15, 7))\n",
    "            \n",
    "            cut_feature = pd.cut(df[feature], bins = bins, labels = labels)\n",
    "            \n",
    "            plt_df = df.groupby(cut_feature).Feedback_RatingValue.mean()\n",
    "                \n",
    "            ax = plt_df.plot(kind = 'bar')\n",
    "                \n",
    "            plt.title(title, fontsize = 16)\n",
    "                \n",
    "            plt.xlabel(xlabel, fontsize = 14)\n",
    "                \n",
    "            plt.ylabel('Average NPS Score', fontsize = 14)\n",
    "\n",
    "            plt.xticks(rotation = 20, fontsize = 12)\n",
    "            \n",
    "            labels = [x for x in pd.DataFrame(pd.DataFrame(cut_feature.value_counts()).iloc[:,0]).reset_index().sort_values('index').iloc[:, 1]]\n",
    "            \n",
    "            driver_analysis.add_value_labels(ax, labels, df)\n",
    "            \n",
    "    def plot_nums(df, feature):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        This function serves as a binner and labeler for the numeric plot structure function above, essentially this is a culmination of my work creating\n",
    "        numeric plots from scratch for a series of other drivers. The features contained in this function are those which have been identified from my \n",
    "        analysis on other projects, and some of them carry over to this one. So that I didn't have to re-create the wheel, I have included them all here and \n",
    "        will be adding more as I go. \n",
    "        \n",
    "        Args: the df that you are in and the feature that you want to plot, better to use this in a loop over a dataframe of the top 10 features\n",
    "        \n",
    "        Output: numerically binned plots\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if feature == 'Device_SysVolSizeMB':\n",
    "        \n",
    "            #plotting System Size MB\n",
    "            driver_analysis.numeric_plot_structure(df, 'Device_SysVolSizeMB',\n",
    "                  'NPS by System Size (GB)', 'System Volume Size (GB)', bins = [0, 30000, 100000, 500000, 1000000, 1000000000000000], \n",
    "                  labels= ['< 30GB', '30GB-100GB', '100GB-500GB', '500GB-1,000GB', '1,000GB+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_ExchangeSubscriptionsCount':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_ExchangeSubscriptionsCount', 'NPS by Exchange Subscription Count', 'Exchange Subscription Count',\n",
    "                                      bins = [0, 4, 5, 6, 8, 9, 10, 149],\n",
    "                                       labels = ['0-4', '4-5', '5-6', '6-8', '8-9', '9-10', '10+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Subscription_Count':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Subscription_Count', 'NPS by Subscription Count', 'Subscription Count',\n",
    "                                      bins = [0, 2, 4, 5, 8, 11, 15, 19, 332],\n",
    "                                       labels = ['0-2', '2-4', '4-5', '5-8', '8-11', '11-15', '15-19', '19-332'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Subscription_Sum_OfferEDU':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Subscription_Sum_OfferEDU', 'NPS by Subscription Sum Offer EDU', 'Subscription Sum Offer EDU',\n",
    "                                      bins = [0, 2, 3, 5, 7, 9, 269],\n",
    "                                       labels = ['0-2', '2-3', '3-5', '5-7', '7-9', '9+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_TrialAvailableUnits':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_TrialAvailableUnits', 'NPS by Trial Available Units', 'Tenant Trial Available Units', \n",
    "                                      bins = [0, 25, 50, 10000, 500000, 1000000, 2000000, 3000000],\n",
    "                                       labels = ['0-25', '25-50', '50-10,000', '10,000-500,000', '500,000-1,000,000', '1-2 million', '2-3 million'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_OfficeAvailableUnits':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_OfficeAvailableUnits', 'NPS by Tenant Office Available Units', 'Office Available Units',\n",
    "                                      bins= [0, 2000, 12000, 60000, 300000, 600000, 750000, 1500000, 100000000000000],\n",
    "                                       labels =['0-2,000', '2,000-12,000', '12,000-60,000', '60,000-300,000', '300,000-600,000', '600,000-750,000', '750,000-1,500,000', '1,500,000+'],\n",
    "                                       method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_TotalSubscriptionsCount':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_TotalSubscriptionsCount', 'NPS by Tenant Total Subscriptions Count', 'Total Subscription Count',\n",
    "                                      bins = [0, 7, 12, 18, 23, 29, 1000],\n",
    "                                       labels = ['0-7', '7-12', '12-18', '18-23', '23-29', '29+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'User_Commercial_TotalAvailableUnits':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, \n",
    "                                                   'User_Commercial_TotalAvailableUnits', 'NPS by Commercial Total Available Units', 'Commercial Total Available Units', \n",
    "                                       bins = [0, 100000, 500000, 750000, 1000000, 1500000, 2000000, 3000000],\n",
    "                                       labels = ['0-100,000', '100,000-500,000', '500,000-750,000', '750,000-1,000,000', \n",
    "                                                 '1,000,000-1,500,000', '1,500,000-2,000,000', '2,000,000-3,000,000'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_ProPlusAvailableUnits':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_ProPlusAvailableUnits', 'NPS by Tenant Pro Plus Available Units', 'Pro Plus Available Units', \n",
    "                                       bins= [0, 100, 1000, 5000, 20000, 50000, 150000, 300000, 100000000000],\n",
    "                                       labels = ['0-100', '100-1,000', '1,000-5,000', '5,000-20,000', '20,000-50,000', '50,000-150,000', '150,000-300,000', '300,000+'],\n",
    "                                       method = 'cut')\n",
    "            \n",
    "        elif feature == 'Device_SysVolFreeSpaceMB':\n",
    "        \n",
    "            #plotting SysVolFreeSpace\n",
    "            driver_analysis.numeric_plot_structure(df, 'Device_SysVolFreeSpaceMB', 'NPS by System Free Space (GB)', 'Free Space (GB)',\n",
    "                          bins = [0, 30000, 60000, 90000, 170000, 200000, 350000, 500000, 750000, 10000000000000], \n",
    "                              labels = ['0-30GB', '30GB-60GB', '60GB-90GB', '90GB-170GB', '170GB-200GB', '200GB-350GB', '350GB-500GB', '500GB-750GB', '750GB+'], method = 'cut')\n",
    "        \n",
    "        elif feature == 'Device_ProcSpeedMHz':\n",
    "        \n",
    "            #plotting Device Processing Speed\n",
    "            driver_analysis.numeric_plot_structure(df, 'Device_ProcSpeedMHz', 'NPS by Device Processing Speed (MHz)', 'Device Processing Speed (MHz)', \n",
    "                     bins = [0, 1500, 2000, 2500, 3000, 3500, 4000, 4500], labels = ['0-1500', '1,500-2,000', '2,000-2,500', '2,500-3,000', '3,000-3,500',\n",
    "                                                                                    '3,500-4,000', '4,000-4,500'], method = 'cut')\n",
    "        \n",
    "        elif feature == 'User_Consumer_TenureDays':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'User_Consumer_TenureDays', 'NPS by Consumer Tenure', 'Consumer Tenure', \n",
    "                      bins = [0, 100, 250, 365, 730, 1095, 1460, 1825, 2190, 2571], labels = ['0-3 Months', '3-9 Months', \n",
    "                    '9 Months-1 Year', '1-2 Years', '2-3 Years', '3-4 Years', '4-5 Years', '5-6 Years', '6+ Years'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Device_HorizontalResolution':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Device_HorizontalResolution', 'NPS by Horizontal Resolution', 'Horizontal Resolution', \n",
    "                     bins = [0, 1400, 1800, 2000, 2500, 5000, 10000],\n",
    "                      labels = ['0-1,400', '1,400-1,800', '1,800-2,000', '2,000-2,500', '2,500-5,000', '5,000+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Device_RamMB':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Device_RamMB', 'Device RAM (GB)', 'Device RAM (GB)', bins = [0, 4000, 8000, 16000, 50000, 100000],\n",
    "                              labels = ['0-40GB', '40GB-80GB', '80GB-160GB', '160GB-500GB', '500GB-1,000GB'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_Odin_TotalUsers':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_Odin_TotalUsers', 'NPS by ODIN Total Users', 'Odin Total Users', \n",
    "                      bins = [0, 5, 50, 150, 750, 5000, 15000, 40000, 100000, 200000, 10000000000000],\n",
    "                          labels = ['0-5', '5-50', '50-150', '150-750', '750-5,000', '5-15,000', '15-40,000', '40-100,000', '100-200,000', '200,000+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_Tenure':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_Tenure', 'NPS by Tenant Tenure', 'Tenant Tenure Days',\n",
    "                          bins = [0, 30, 90, 365, 730, 1095, 1460, 1825, 2190, 2555, 2920],\n",
    "                          labels = ['0-1 month', '1-3 months', '3 months-1 year', '1-2 years', '2-3 years', '3-4 years', '4-5 years', '5-6 years', '6-7 years', '7-8 years'], \n",
    "                                                   method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_PaidAvailableUnits':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_PaidAvailableUnits', 'NPS by Paid Available Units', 'Available Units', \n",
    "                  bins = [0, 5, 30, 100, 300, 2500, 10000, 30000, 200000, 10000000000000000],\n",
    "                  labels = ['0-5', '5-30', '30-100', '100-300', '300-2,500', '1,500-10,000', '10-30,000', '30-200,000', '200,000+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_EXOEnabledUsers':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_EXOEnabledUsers', 'NPS by EXO Enabled Users', 'Enabled Users', \n",
    "                      bins = [0, 10, 50, 200, 1000, 5000, 10000, 50000, 150000, 10000000000000],\n",
    "                      labels = ['0-10', '10-50', '50-200', '200-1,000', '1-5,000', '5-10,000', '10-50,000', '50-150,000', '150,000+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_LYOEnabledUsers':\n",
    "        \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_LYOEnabledUsers', 'NPS by LYO Enabled Users', 'Enabled Users', \n",
    "                      bins = [0, 12, 55, 400, 3000, 15000, 70000, 1000000000000],\n",
    "                      labels = ['0-12', '12-55', '55-400', '400-3,000','3-15,000', '15-70,000', '70,000+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'User_Commercial_TotalEnabledUsers':\n",
    "        \n",
    "            driver_analysis.numeric_plot_structure(df, 'User_Commercial_TotalEnabledUsers', 'NPS by Commercial Total Enabled Users', 'Total Enabled Users', \n",
    "                      bins = [0, 20, 100, 300, 2000, 10000, 30000, 80000, 200000, 1000000000000],\n",
    "                      labels = ['0-20', '20-100', '100-300', '300-2,000', '2-10,000', '10-30,000', '30-80,000', '80-200,000', '200,000+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_TotalEnabledUsers':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_TotalEnabledUsers', 'NPS by Total Enabled Users', 'Total Enabled Users', \n",
    "                      bins = [0, 20, 100, 250, 1000, 5000, 20000, 50000, 100000, 250000, 100000000000000],\n",
    "                      labels = ['0-20', '20-100', '100-250', '250-1,000', '1-5,000', '5-20,000', '20-50,000', '50-100,000', '100-250,000', '250,000+'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_SPOEnabledUsers':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_SPOEnabledUsers', 'NPS by SPO Enabled Users', 'SPO Enabled Users', \n",
    "                 bins = [0, 10, 100, 1000, 5000, 10000, 15000, 30000, 75000, 150000, 1000000000],\n",
    "                  labels = ['0-10', '10-100', '100-1,000', '1-5,000', '5-10,000', '10-15,000', '15-30,000', '30-75,000', '75-150,000', '150,000+'], method = 'cut')\n",
    "        \n",
    "        elif feature == 'Tenant_OD4BAvailableUnits':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_OD4BAvailableUnits', 'NPS by Tenant_OD4BAvailableUnits', 'Available Units', \n",
    "                                       bins = [0, 10, 5000, 20000, 50000, 100000, 200000, 500000, 1000000, 1500000, 30000000],\n",
    "                                       labels = ['0-10', '10 - 5,000', '5 - 20,000', '20 - 50,000', '50 - 100,000', '100 - 200,000', \n",
    "                                                '200 - 500,000', '500,000 - 1 million', '1 - 1.5 million', '1.5 million +'], method = 'cut')\n",
    "            \n",
    "        elif feature == 'Tenant_NonTrialSubscriptionsCount':\n",
    "            \n",
    "            driver_analysis.numeric_plot_structure(df, 'Tenant_NonTrialSubscriptionsCount', 'NPS by Tenant_NonTrialSubscriptionsCount', 'Subscriptions Count', \n",
    "                                       bins = [0, 7, 10, 12, 15, 18, 22, 25, 30, 40, 5000],\n",
    "                                      labels = ['0-7', '7 - 10', '10 - 12', '12 - 15', '15 - 18', '18 - 22', '22 - 25', '25 - 30', '30 - 40', '40 +'], method = 'cut')\n",
    "            \n",
    "                \n",
    "    def plot_promoter_and_detractor_percentages(df):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        This function plots the percentage of each group's detractors, passives, and promoters\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        for feature in df.columns:\n",
    "    \n",
    "            if (df[feature].nunique() > 40) & (feature != 'Feedback_RatingValue'):\n",
    "            \n",
    "                top_20 = pd.DataFrame(df[feature].value_counts().nlargest(20)).index\n",
    "            \n",
    "                plt_df = df[df[feature].isin(top_20)]\n",
    "            \n",
    "                counts = (plt_df.groupby(['Feedback_RatingValue'])[feature].value_counts(normalize = True).rename('percentage').mul(100).reset_index())\n",
    "            \n",
    "                plt.figure(figsize = (25, 8))\n",
    "            \n",
    "                ax = sns.barplot(x = feature, y = 'percentage', hue = counts['Feedback_RatingValue'].map({-100:'Detractors', 0:'Passives', 100:'Promoters'}), data = counts,\n",
    "                        order = counts.iloc[counts[counts['Feedback_RatingValue'] == 100]['percentage'].sort_values().index][feature])\n",
    "            \n",
    "                plt.title(f'Top 20 {feature} Detractors, Passives, and Promoters', fontsize = 22)\n",
    "            \n",
    "                plt.xticks(rotation = 65, fontsize = 12)\n",
    "                \n",
    "                plt.xlabel(f'{feature}', fontsize = 14)\n",
    "                \n",
    "                plt.ylabel('Percentage', fontsize = 14)\n",
    "                \n",
    "                plt.legend(fontsize = 16)\n",
    "\n",
    "            \n",
    "            elif (df[feature].nunique() < 40) & (feature != 'Feedback_RatingValue'):\n",
    "            \n",
    "                counts = (df.groupby(['Feedback_RatingValue'])[feature].value_counts(normalize = True).rename('percentage').mul(100).reset_index())\n",
    "            \n",
    "                plt.figure(figsize = (25, 8))\n",
    "            \n",
    "                ax = sns.barplot(x = feature, y = 'percentage', hue = counts['Feedback_RatingValue'].map({-100:'Detractors', 0:'Passives', 100:'Promoters'}), data = counts,\n",
    "                        order = counts.iloc[counts[counts['Feedback_RatingValue'] == 100]['percentage'].sort_values().index][feature])\n",
    "            \n",
    "                plt.title(f'{feature} Detractors, Passives, and Promoters', fontsize = 22)\n",
    "            \n",
    "                plt.xticks(rotation = 65, fontsize = 12)\n",
    "                \n",
    "                plt.xlabel(f'{feature}', fontsize = 14)\n",
    "                \n",
    "                plt.ylabel('Percentage', fontsize = 14)\n",
    "                \n",
    "                plt.legend(fontsize = 16)\n",
    "                \n",
    "    def plot_build_age(nps, df, ages, subset, bins, labels):\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        Becuase we want to look at office build in terms of the age of the build, we must enter them in here, this function plots ages of builds, cut up into \n",
    "        the appropriate bins\n",
    "    \n",
    "        '''\n",
    "\n",
    "        #needed to go online to get age of build in months, then match those to the exact build\n",
    "        office_builds = pd.DataFrame(df.Device_OfficeBuild.value_counts().nlargest(20)).reset_index().rename(columns = {'index':'Device_OfficeBuild'})\n",
    "        \n",
    "        office_builds['months_old'] = ages\n",
    "\n",
    "        build_nps = pd.DataFrame(df.groupby('Device_OfficeBuild').Feedback_RatingValue.agg({'mean', 'size'})).sort_values('size', ascending=False)[:20].reset_index()\n",
    "\n",
    "        build_nps['age'] = [x for x in office_builds['months_old']]\n",
    "\n",
    "        build_nps['age'] = pd.cut(build_nps['age'], bins= bins, labels = labels)\n",
    "    \n",
    "        plt.figure(figsize = (12, 7))\n",
    "        \n",
    "        #sns.barplot(x = 'age', y = 'mean', data = build_nps, ci = False)\n",
    "        \n",
    "        build_nps.groupby('age')['mean'].mean().plot(kind = 'bar')\n",
    "        \n",
    "        plt.title('NPS by Build Age', fontsize = 22)\n",
    "        \n",
    "        plt.xlabel('Build Age (in months)', fontsize = 16)\n",
    "        \n",
    "        plt.ylabel('NPS', fontsize = 16)\n",
    "        \n",
    "        plt.axhline(y=nps, linestyle = '--', color = 'tab:blue')\n",
    "        \n",
    "        if subset is 'windows':\n",
    "            \n",
    "            leg = plt.legend(labels = ['2020 AVG Windows NPS'])\n",
    "            \n",
    "        elif subset is 'web':\n",
    "            \n",
    "            leg = plt.legend(labels = ['2020 AVG Web NPS'])\n",
    "            \n",
    "        leg_lines = leg.get_lines()\n",
    "        \n",
    "        plt.setp(leg_lines, linestyle = '--', color = 'tab:blue', linewidth = 2)\n",
    "        \n",
    "        plt.xticks(rotation = 0)\n",
    "        \n",
    "    def generate_nps_trends():\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        This function takes a list of filepaths, included in the body of the function here, iterates through each filepath, and plots lineplot nps trends for each\n",
    "        of these filepaths. So in each filepath, there should be a separate csv file for each segment (i.e. in Word, there should be a 'windows_nps', 'web_nps',\n",
    "        'windows_consumer_nps', 'windows_comedu_nps', 'windows_nonedu_nps', 'web_comedu_nps', 'web_nonedu_nps'). If each directory contains those files, then this function\n",
    "        will work perfectly, if the filepaths are not set up that way then it wont. Each of these csv files in the filepath should have three columns (as taken from kusto\n",
    "        using the query below), 1. 'avg_nps', 2. 'year', 3. 'month' -- this structure will show the average nps score of the product by year and month\n",
    "        \n",
    "        ARGS: There are no arguments for this function, simply write it out\n",
    "        \n",
    "        Output: a series of plots that show NPS trends over time (lineplots)\n",
    "        \n",
    "        Kusto Query:\n",
    "        \n",
    "        #this first query is for the app overall, either web or windows desktop\n",
    "        \n",
    "        SELECT AVG(CAST(Feedback_RatingValue AS FLOAT)) AS avg_nps, DATEPART(year, Feedback_DateTime) AS year, DATEPART(month, Feedback_DateTime)\n",
    "        AS month FROM EndUserAppNPS()\n",
    "        WHERE Feedback_Source LIKE '%{product}%'\n",
    "        AND Feedback_Platform LIKE '%Windows%' AND Feedback_DateTime >= CAST('2019-05-01' AS datetime)\n",
    "        GROUP BY DATEPART(year, Feedback_DateTime), DATEPART(month, Feedback_DateTime)\n",
    "        ORDER BY year, month\n",
    "        \n",
    "        #this query can be adapted for commercial edu and commercial nonedu\n",
    "        \n",
    "        SELECT AVG(CAST(Feedback_RatingValue AS FLOAT)) AS avg_nps, \n",
    "        DATEPART(year, Feedback_DateTime) AS year, DATEPART(month, Feedback_DateTime)\n",
    "        AS month FROM EndUserAppNPS()\n",
    "        WHERE Feedback_UserType = 'Commercial' AND User_Commercial_HasEducation = 1 AND Feedback_Source LIKE '%Teams%' \n",
    "        AND Feedback_Platform LIKE '%Windows%' AND Feedback_DateTime >= CAST('2019-05-01' AS datetime)\n",
    "        GROUP BY DATEPART(year, Feedback_DateTime), DATEPART(month, Feedback_DateTime)\n",
    "        ORDER BY year, month\n",
    "        \n",
    "        #this one is for consumer\n",
    "        \n",
    "        SELECT AVG(CAST(Feedback_RatingValue AS FLOAT)) AS avg_nps, DATEPART(year, Feedback_DateTime) \n",
    "        AS year, DATEPART(month, Feedback_DateTime)\n",
    "        AS month FROM EndUserAppNPS()\n",
    "        WHERE Feedback_UserType = 'Consumer' AND Feedback_Source LIKE '%Word%' AND Feedback_Platform LIKE '%Windows%'\n",
    "        AND Feedback_DateTime >= CAST('2019-05-01' AS datetime)\n",
    "        GROUP BY DATEPART(year, Feedback_DateTime), DATEPART(month, Feedback_DateTime)\n",
    "        ORDER BY year, month\n",
    "        \n",
    "        #this last one is the kusto query for windows desktop or web in full suite NPS\n",
    "        \n",
    "        EndUserO365NPS(platform = 'Web')\n",
    "        |where Feedback_DateTime >= datetime('2019-05-01')\n",
    "        |summarize avg_nps = avg(Feedback_RatingValue) by year = getyear(Feedback_DateTime), month = getmonth(Feedback_DateTime)\n",
    "        |order by year asc, month asc\n",
    "        \n",
    "        #for suite consumer\n",
    "        \n",
    "        EndUserO365NPS(platform = 'Web')\n",
    "        |where Feedback_DateTime >= datetime('2019-05-01') and Feedback_UserType == 'Consumer'\n",
    "        |summarize avg_nps = avg(Feedback_RatingValue) by year = getyear(Feedback_DateTime), month = getmonth(Feedback_DateTime)\n",
    "        |order by year asc, month asc\n",
    "        \n",
    "        #for suite commercial edu or nonedu\n",
    "        \n",
    "        EndUserO365NPS(platform = 'Windows Desktop')\n",
    "        |where Feedback_DateTime >= datetime('2019-05-01') and Feedback_UserType == 'Commercial' and User_Commercial_HasEducation == 1\n",
    "        |summarize avg_nps = avg(Feedback_RatingValue) by year = getyear(Feedback_DateTime), month = getmonth(Feedback_DateTime)\n",
    "        |order by year asc, month asc\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        filepaths = ['C:\\\\Users\\\\fulto\\\\Desktop\\\\Driver Analysis\\\\Suite', 'C:\\\\Users\\\\fulto\\Desktop\\\\Driver Analysis\\\\Excel', 'C:\\\\Users\\\\fulto\\\\Desktop\\\\Driver Analysis\\\\Word',\n",
    "            'C:\\\\Users\\\\fulto\\\\Desktop\\\\Driver Analysis\\\\PP', 'C:\\\\Users\\\\fulto\\\\Desktop\\\\Driver Analysis\\\\Outlook', 'C:\\\\Users\\\\fulto\\\\Desktop\\\\Driver Analysis\\\\Teams']\n",
    "    \n",
    "        from itertools import cycle\n",
    "        \n",
    "        lines = ['-', '-.', '--', ':']\n",
    "        \n",
    "        linecycler = cycle(lines)\n",
    "    \n",
    "        for filepath in filepaths:\n",
    "        \n",
    "            if filepath.endswith('Teams'):\n",
    "    \n",
    "                for filename in os.listdir(filepath):\n",
    "        \n",
    "                    os.chdir(filepath)\n",
    "        \n",
    "                    if filename.startswith('windows'):\n",
    "            \n",
    "                        df = pd.read_csv(f'{filename}')\n",
    "            \n",
    "                        ax = df['avg_nps'].plot(figsize = (18, 10), linewidth = 4, linestyle = next(linecycler))\n",
    "            \n",
    "                        plt.title(f'{filepath[39:]} Windows 10 Month NPS Trend', fontsize = 22)\n",
    "            \n",
    "                        plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ['Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'March', \n",
    "                                                           'April', 'May', 'June'], fontsize = 16, rotation = 15)\n",
    "                \n",
    "                        ax.legend(fontsize = 'xx-large', labels = ['Commercial Edu', 'Commercial Non-Edu',  'Overall Windows'])\n",
    "                    \n",
    "                        plt.xlabel('NPS Trend (2019 - 2020)', fontsize = 16)\n",
    "                    \n",
    "                        plt.ylabel('NPS', fontsize = 16)\n",
    "            \n",
    "                plt.show()\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                for filename in os.listdir(filepath):\n",
    "                \n",
    "                    os.chdir(filepath)\n",
    "        \n",
    "                    if filename.startswith('windows'):\n",
    "            \n",
    "                        df = pd.read_csv(f'{filename}')\n",
    "            \n",
    "                        ax = df['avg_nps'].plot(figsize = (18, 10), linewidth = 4, linestyle = next(linecycler))\n",
    "            \n",
    "                        plt.title(f'{filepath[39:]} Windows 13 Month NPS Trend', fontsize = 22)\n",
    "            \n",
    "                        plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], ['May', 'June', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'March', \n",
    "                                                           'April', 'May', 'June'], fontsize = 16, rotation = 15)\n",
    "                \n",
    "                        ax.legend(fontsize = 'xx-large', labels = ['Commercial Edu', 'Consumer', 'Commercial Non-Edu', 'Overall Windows'])\n",
    "                    \n",
    "                        plt.xlabel('NPS Trend (2019 - 2020)', fontsize = 16)\n",
    "                    \n",
    "                        plt.ylabel('NPS', fontsize = 16)\n",
    "                \n",
    "                plt.show()\n",
    "    \n",
    "        for filepath in filepaths:\n",
    "        \n",
    "            if filepath.endswith('Teams'):\n",
    "            \n",
    "                for filename in os.listdir(filepath):\n",
    "        \n",
    "                    os.chdir(filepath)\n",
    "        \n",
    "                    if filename.startswith('web'):\n",
    "            \n",
    "                        df = pd.read_csv(f'{filename}')\n",
    "            \n",
    "                        ax = df['avg_nps'].plot(figsize = (18, 10), linewidth = 4, linestyle = next(linecycler))\n",
    "            \n",
    "                        plt.title(f'{filepath[39:]} Web 10 Month NPS Trend', fontsize = 22)\n",
    "            \n",
    "                        plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ['Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'March', \n",
    "                                                           'April', 'May', 'June'], fontsize = 16, rotation = 15)\n",
    "                    \n",
    "                        ax.legend(fontsize = 'xx-large', labels = ['Commercial Edu', 'Commercial Non-Edu', 'Overall Web'])\n",
    "                    \n",
    "                        plt.xlabel('NPS Trend (2019 - 2020)', fontsize = 16)\n",
    "                    \n",
    "                        plt.ylabel('NPS', fontsize = 16)\n",
    "            \n",
    "                plt.show()\n",
    "    \n",
    "            else:\n",
    "            \n",
    "                for filename in os.listdir(filepath):\n",
    "        \n",
    "                    os.chdir(filepath)\n",
    "        \n",
    "                    if filename.startswith('web'):\n",
    "            \n",
    "                        df = pd.read_csv(f'{filename}')\n",
    "            \n",
    "                        ax = df['avg_nps'].plot(figsize = (18, 10), linewidth = 4, linestyle = next(linecycler))\n",
    "            \n",
    "                        plt.title(f'{filepath[39:]} Web 13 Month NPS Trend', fontsize = 22)\n",
    "            \n",
    "                        plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], ['May', 'June', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'March', \n",
    "                                                           'April', 'May', 'June'], fontsize = 16, rotation = 15)\n",
    "                \n",
    "                        ax.legend(fontsize = 'xx-large', labels = ['Commercial Edu', 'Consumer', 'Commercial Non-Edu', 'Overall Web'])\n",
    "                    \n",
    "                        plt.xlabel('NPS Trend (2019 - 2020)', fontsize = 16)\n",
    "                    \n",
    "                        plt.ylabel('NPS', fontsize = 16)\n",
    "            \n",
    "                plt.show()\n",
    "                \n",
    "    def get_top_10_feats(df, web_or_windows = None, subset = None):\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        This function takes an input of a dataframe of feature importances from the modeling function, then it subsets the df for those features only, returning a \n",
    "        df of 10 features and NPS scores\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        top_10 = [x for x in df.iloc[:10, 0].reset_index()['Features']]\n",
    "    \n",
    "        new_df = web_or_windoes[web_or_windows['NgramThemePredictions'] == subset]\n",
    "    \n",
    "        new_df = pd.concat([new_df[top_10], new_df['Feedback_RatingValue']], axis = 1)\n",
    "    \n",
    "        return new_df\n",
    "    \n",
    "    def web_and_windows_slicer(filepath, csv):\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        This function takes a filepath and a csv filename as arguments and slices that df up into web and windows datasets, this is useful if you are performing analyses on \n",
    "        web and windows dfs only, it provides you with 6 dfs that you can name whatever you want that will be 3 web and 3 windows dfs, one each for consumer,\n",
    "        commercial edu, and commercial nonedu, if your dataset is missing windows or web data then it will simply result in an empty df, but you must have 6 dfs for it to \n",
    "        unpack into or else it will throw an error\n",
    "    \n",
    "        Args: filepath = filepath where csv file is contained, csv = name of csv file in that filepath\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        os.chdir(filepath)\n",
    "    \n",
    "        df = pd.read_csv(csv, low_memory = False)\n",
    "    \n",
    "        web = df[df['Feedback_Platform'].str.contains('Web')]\n",
    "    \n",
    "        windows = df[df['Feedback_Platform'].str.contains('Windows')]\n",
    "    \n",
    "        web_consumer = web[web['Feedback_UserType'] == 'Consumer']\n",
    "    \n",
    "        web_com = web[web['Feedback_UserType'] == 'Commercial']\n",
    "    \n",
    "        web_comedu = web_com[web_com['User_Commercial_HasEducation'] == 1.0]\n",
    "    \n",
    "        web_nonedu = web_com[web_com['User_Commercial_HasEducation'] == 0.0]\n",
    "    \n",
    "        windows_consumer = windows[windows['Feedback_UserType'] == 'Consumer']\n",
    "                        \n",
    "        windows_com = windows[windows['Feedback_UserType'] == 'Commercial']\n",
    "                        \n",
    "        windows_comedu = windows_com[windows_com['User_Commercial_HasEducation'] == 1.0]\n",
    "                        \n",
    "        windows_nonedu = windows_com[windows_com['User_Commercial_HasEducation'] == 0.0]\n",
    "                        \n",
    "        dfs = [web_consumer, web_comedu, web_nonedu, windows_consumer, windows_comedu, windows_nonedu]\n",
    "                        \n",
    "        dfs = [driver_analysis.remove_completely_missing_columns(df, .9) for df in dfs]\n",
    "                        \n",
    "        web_consumer, web_comedu, web_nonedu, windows_consumer, windows_comedu, windows_nonedu = dfs\n",
    "                        \n",
    "        return web_consumer, web_comedu, web_nonedu, windows_consumer, windows_comedu, windows_nonedu\n",
    "    \n",
    "    def read_entire_csv(filepath, filename):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        This function reads in a csv without slicing it up into consumer, commercial, at all and removes features with a high amount of missingness.\n",
    "        This function is most useful for running an analysis on an entire dataset such as suite web, suite windows, etc.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        import os\n",
    "        \n",
    "        os.chdir(filepath)\n",
    "        \n",
    "        df = pd.read_csv(filename, low_memory = False, encoding='latin-1')\n",
    "        \n",
    "        df = driver_analysis.remove_completely_missing_columns(df, .75)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_detractors_and_promoters(filepath, csv):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        This function splits up web and windows dfs (if they both exist) into detractors, passives, and promoters so that we can run the driver analysis,\n",
    "        or plot these subsets, in more fine-grained detail. The arguments for this function are the same as the previosu function. This function returns a list\n",
    "        of dfs that needs to be unpacked upon use of the function. i.e. df1, df2, df3 = dfs\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        os.chdir(filepath)\n",
    "    \n",
    "        df = pd.read_csv(csv, low_memory = False)\n",
    "    \n",
    "        web = df[df['Feedback_Platform'].str.contains('Web')]\n",
    "    \n",
    "        windows = df[df['Feedback_Platform'].str.contains('Windows')]\n",
    "    \n",
    "        web_consumer = web[web['Feedback_UserType'] == 'Consumer']\n",
    "    \n",
    "        web_com = web[web['Feedback_UserType'] == 'Commercial']\n",
    "    \n",
    "        web_comedu = web_com[web_com['User_Commercial_HasEducation'] == 1.0]\n",
    "    \n",
    "        web_nonedu = web_com[web_com['User_Commercial_HasEducation'] == 0.0]\n",
    "    \n",
    "        windows_consumer = windows[windows['Feedback_UserType'] == 'Consumer']\n",
    "                        \n",
    "        windows_com = windows[windows['Feedback_UserType'] == 'Commercial']\n",
    "                        \n",
    "        windows_comedu = windows_com[windows_com['User_Commercial_HasEducation'] == 1.0]\n",
    "                        \n",
    "        windows_nonedu = windows_com[windows_com['User_Commercial_HasEducation'] == 0.0]\n",
    "                        \n",
    "        dfs = [web_consumer, web_comedu, web_nonedu, windows_consumer, windows_comedu, windows_nonedu]\n",
    "                        \n",
    "        dfs = [driver_analysis.remove_completely_missing_columns(df, .75) for df in dfs]\n",
    "            \n",
    "        for df in dfs:\n",
    "                \n",
    "            web_consumer_detractors = web_consumer[web_consumer['Feedback_RatingValue'] == -100]\n",
    "            \n",
    "            web_consumer_passives = web_consumer[web_consumer['Feedback_RatingValue'] == 0]\n",
    "            \n",
    "            web_consumer_promoters = web_consumer[web_consumer['Feedback_RatingValue'] == 100]\n",
    "            \n",
    "            web_comedu_detractors = web_comedu[web_comedu['Feedback_RatingValue'] == -100]\n",
    "            \n",
    "            web_comedu_passives = web_comedu[web_comedu['Feedback_RatingValue'] == 0]\n",
    "            \n",
    "            web_comedu_promoters = web_comedu[web_comedu['Feedback_RatingValue'] == 100]\n",
    "            \n",
    "            web_nonedu_detractors = web_nonedu[web_nonedu['Feedback_RatingValue'] == -100]\n",
    "            \n",
    "            web_nonedu_passives = web_nonedu[web_nonedu['Feedback_RatingValue'] == 0]\n",
    "            \n",
    "            web_nonedu_promoters = web_nonedu[web_nonedu['Feedback_RatingValue'] == 100]\n",
    "            \n",
    "            windows_consumer_detractors = windows_consumer[windows_consumer['Feedback_RatingValue'] == -100]\n",
    "            \n",
    "            windows_consumer_passives = windows_consumer[windows_consumer['Feedback_RatingValue'] == 0]\n",
    "            \n",
    "            windows_consumer_promoters = windows_consumer[windows_consumer['Feedback_RatingValue'] == 100]\n",
    "            \n",
    "            windows_comedu_detractors = windows_comedu[windows_comedu['Feedback_RatingValue'] == -100]\n",
    "            \n",
    "            windows_comedu_passives = windows_comedu[windows_comedu['Feedback_RatingValue'] == 0]\n",
    "            \n",
    "            windows_comedu_promoters = windows_comedu[windows_comedu['Feedback_RatingValue'] == 100]\n",
    "            \n",
    "            windows_nonedu_detractors = windows_nonedu[windows_nonedu['Feedback_RatingValue'] == -100]\n",
    "            \n",
    "            windows_nonedu_passives = windows_nonedu[windows_nonedu['Feedback_RatingValue'] == 0]\n",
    "            \n",
    "            windows_nonedu_promoters = windows_nonedu[windows_nonedu['Feedback_RatingValue'] == 100]\n",
    "            \n",
    "        if web_consumer_detractors.shape[1] == 0:\n",
    "            \n",
    "            dfs = [windows_consumer_detractors, windows_consumer_passives, windows_consumer_promoters, windows_comedu_detractors, windows_comedu_passives,\n",
    "                  windows_comedu_promoters, windows_nonedu_detractors, windows_nonedu_passives, windows_nonedu_promoters]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            dfs = [web_consumer_detractors, web_consumer_passives, web_consumer_promoters, web_comedu_detractors, web_comedu_passives, web_comedu_promoters,\n",
    "                  web_nonedu_detractors, web_nonedu_passives, web_nonedu_promoters, windows_consumer_detractors, windows_consumer_passives, windows_consumer_promoters, \n",
    "                  windows_comedu_detractors, windows_comedu_passives, windows_comedu_promoters, windows_nonedu_detractors, windows_nonedu_passives, windows_nonedu_promoters]\n",
    "            \n",
    "        return dfs       \n",
    "    \n",
    "    #model with LightGBM -- which accepts missingness with no special adaptation, this will truncate this process significantly\n",
    "    def find_drivers(df, title, throw_out_features = None, binarize_target = None, tune_params = None):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        This function mean target encodes a set of features, then runs them through two separate Light GBM models, the first extracts all feature importances\n",
    "        and the second uses the top 10 features from the first to run a model that returns the feature importances for those ten variables controlling for\n",
    "        one another (using the # of times that the decision trees split on that variable). The final output of this function are two plots of feature importances, along with \n",
    "        the df containing a subset of the data for the top 10 features.\n",
    "        \n",
    "        Args: arguments in this function are a df and the title of the feature importances plot that you want\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #some of these dfs might not have \"Feedback_Rating\" in them\n",
    "        if 'Feedback_Rating' in df.columns:\n",
    "            \n",
    "            features = df.drop(['Feedback_RatingValue', 'Feedback_Rating'], axis = 1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            features = df.drop('Feedback_RatingValue', axis = 1)\n",
    "        \n",
    "        target = df['Feedback_RatingValue']\n",
    "        \n",
    "        if binarize_target is not None:\n",
    "            \n",
    "            target = df['Feedback_RatingValue'].map({-100:0, 0:0, 100:1})\n",
    "        \n",
    "    \n",
    "        #import lightgbm\n",
    "        import lightgbm as lgb\n",
    "        \n",
    "        if throw_out_features is not None:\n",
    "        \n",
    "            combined = driver_analysis.throw_out_useless_features(pd.concat([features, target], axis = 1))\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            combined = pd.concat([features, target], axis = 1)\n",
    "        \n",
    "        for col in combined.columns:\n",
    "            \n",
    "            if combined[col].dtype == 'O':\n",
    "                \n",
    "                combined[col] = driver_analysis.target_encoder(combined, column = col, target = 'Feedback_RatingValue', index = None, method = 'mean')\n",
    "    \n",
    "                encoded_features = combined.drop('Feedback_RatingValue', axis = 1)\n",
    "    \n",
    "                target = combined['Feedback_RatingValue']\n",
    "        \n",
    "        if pd.DataFrame(target).iloc[:, 0].nunique() > 2:\n",
    "    \n",
    "            #instantiate model\n",
    "            LGBMModel = lgb.LGBMClassifier(objective = 'multiclass').fit(encoded_features, target)\n",
    "            \n",
    "        elif pd.DataFrame(target).iloc[:, 0].nunique() == 2:\n",
    "            \n",
    "            #instantiate model\n",
    "            LGBMModel = lgb.LGBMClassifier(objective = 'binary').fit(encoded_features, target)\n",
    "        \n",
    "        \n",
    "#         ----This code below performs a gridsearch if you want it---\n",
    "        if tune_params is not None:\n",
    "    \n",
    "            param_grid = {'n_estimators': [50, 100, 250, 500, 750, 1000, 1250, 1500],#'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                  'max_depth': [2,5,10,25,None]}\n",
    "    \n",
    "            CV_lgb = RandomizedSearchCV(estimator=LGBMModel, n_jobs=-1, param_distributions=param_grid, verbose=10, scoring='neg_log_loss',cv= 5)\n",
    "    \n",
    "            CV_lgb.fit(encoded_features, target)\n",
    "        \n",
    "            n_estimators, max_depth = CV_lgb.best_params_.values()\n",
    "            \n",
    "            if binarize_target is not None:\n",
    "\n",
    "                LGBMModel = lgb.LGBMClassifier(random_state = 0, n_estimators = n_estimators, max_depth = max_depth, objective = 'binary').fit(encoded_features, target)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                LGBMModel = lgb.LGBMClassifier(random_state = 0, n_estimators = n_estimators, max_depth = max_depth, objective = 'multiclass').fit(encoded_features, target)\n",
    "\n",
    "        #getting feature importances greatest to least\n",
    "        learners = LGBMModel.feature_importances_.argsort()[::-1]\n",
    "    \n",
    "        features_df = pd.DataFrame(encoded_features.columns[learners], LGBMModel.feature_importances_[learners])\n",
    "    \n",
    "        #only using features that had more than .015 splits\n",
    "        feature_importances = features_df[features_df.index>0.005]\n",
    "    \n",
    "        #get top 10 features from feature importances to be fed into the next model\n",
    "        top_10 = [x for x in feature_importances.iloc[:10, 0]]\n",
    "    \n",
    "        #subset features by the top 10 -- this begins the second stage of modeling\n",
    "        encoded_features = encoded_features[top_10]\n",
    "        \n",
    "        if pd.DataFrame(target).iloc[:,0].nunique() > 2:\n",
    "    \n",
    "            #instantiate model\n",
    "            LGBMModel = lgb.LGBMClassifier(objective = 'multiclass').fit(encoded_features, target)\n",
    "            \n",
    "        elif pd.DataFrame(target).iloc[:, 0].nunique() == 2:\n",
    "            \n",
    "            #instantiate model\n",
    "            LGBMModel = lgb.LGBMClassifier(objective = 'binary').fit(encoded_features, target)\n",
    "        \n",
    "        import shap\n",
    "        \n",
    "        shap_values = shap.TreeExplainer(LGBMModel).shap_values(encoded_features)\n",
    "        \n",
    "        plt.figure(figsize = (12, 7))\n",
    "        shap.summary_plot(shap_values, encoded_features, plot_type=\"bar\")\n",
    "        \n",
    "        #----again, this code below can be used for a second tuning session----\n",
    "        if tune_params is not None:\n",
    "        \n",
    "            param_grid = {'n_estimators': [50, 100, 250, 500, 750, 1000, 1250, 1500],#'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                  'max_depth': [2,5,10,25,None]}\n",
    "    \n",
    "            CV_lgb = RandomizedSearchCV(estimator=LGBMModel, n_jobs=-1, param_distributions=param_grid, verbose=10, scoring='neg_log_loss',cv= 5)\n",
    "    \n",
    "            CV_lgb.fit(encoded_features, target)\n",
    "        \n",
    "            n_estimators, max_depth = CV_lgb.best_params_.values()\n",
    "            \n",
    "            if binarize_target is not None:\n",
    "\n",
    "                LGBMModel = lgb.LGBMClassifier(random_state = 0, n_estimators = n_estimators, max_depth = max_depth, objective = 'binary').fit(encoded_features, target)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                LGBMModel = lgb.LGBMClassifier(random_state = 0, n_estimators = n_estimators, max_depth = max_depth, objective = 'multiclass').fit(encoded_features, target)\n",
    "\n",
    "        #getting feature importances greatest to least\n",
    "        learners = LGBMModel.feature_importances_.argsort()[::-1]\n",
    "        \n",
    "        features_df = pd.DataFrame(encoded_features.columns[learners], LGBMModel.feature_importances_[learners])\n",
    "    \n",
    "        #only using features that had more than .005 splits\n",
    "        feature_importances = encoded_features[encoded_features.index>0.005]\n",
    "        \n",
    "        important_df = pd.concat([features[top_10], target], axis = 1)\n",
    "    \n",
    "        importance_plot = lgb.plot_importance(LGBMModel, figsize = (12, 8), importance_type = 'split', ignore_zero = True, grid = False, title = title)\n",
    "        \n",
    "        features_df = features_df.iloc[:10, :]\n",
    "\n",
    "        features_df.rename(columns = {0:'one'}, inplace = True)\n",
    "\n",
    "        features_df.reset_index(inplace = True)\n",
    "\n",
    "        features_df.rename(columns = {'index':'two'}, inplace = True)\n",
    "        \n",
    "        #create plot that uses feature importance splits to create percentage, per Manoj's request\n",
    "        features_df['two'] = [x/sum(features_df['two']) for x in features_df['two']]\n",
    "        \n",
    "        plt.figure(figsize = (12, 7))\n",
    "        ax = sns.barplot(y = 'one', x = 'two', data = features_df, color = 'tab:blue') \n",
    "        plt.title(title, fontsize = 22)\n",
    "        plt.xlabel('Importance Meter! (How Often a Variable was Used by the Model)', fontsize = 18)\n",
    "        plt.ylabel('Features', fontsize = 18)\n",
    "    \n",
    "        return pd.DataFrame(important_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
