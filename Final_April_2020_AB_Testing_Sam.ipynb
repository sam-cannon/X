{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sklearn.tree\n",
    "\n",
    "from IPython.display import SVG\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_classif, mutual_info_classif, chi2,SelectPercentile,SelectFdr,SelectFpr,SelectFromModel\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV, LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Data Import\n",
    "- I am going to import 2 dfs, one that had the data (with no column names) and another that contains the correct column names (necessary for analysis ;)) \n",
    "- I am going to read the larger df in by chunks, since this df is over 6,000,000 rows and won't import in one fell swoop onto my machine\n",
    "- Each chunk will replace its column names with the proper names\n",
    "- Each chunk will be truncated into only information pertinent to the AB analysis, primarily:\n",
    "    - XpId\n",
    "    - XpGroupType\n",
    "    - XpUrl\n",
    "    - Id\n",
    "    - SureyRating\n",
    "    - ProcessSessionId\n",
    "    - Platform\n",
    "    - Product\n",
    "    - SurveyRatingQuestion\n",
    "- After this data import, I will be translating the questions into english to use in tagging NPS questions as suite or app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 1\n",
      "finished importing chunk 2\n",
      "finished importing chunk 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (30,170,181) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 4\n",
      "finished importing chunk 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (170,181) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,30,170,181,205,357) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 7\n",
      "finished importing chunk 8\n",
      "finished importing chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (30,170,181,357) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 10\n",
      "finished importing chunk 11\n",
      "finished importing chunk 12\n",
      "finished importing chunk 13\n",
      "finished importing chunk 14\n",
      "finished importing chunk 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,170,181,205) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 16\n",
      "finished importing chunk 17\n",
      "finished importing chunk 18\n",
      "finished importing chunk 19\n",
      "finished importing chunk 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (170,181,357) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 21\n",
      "finished importing chunk 22\n",
      "finished importing chunk 23\n",
      "finished importing chunk 24\n",
      "finished importing chunk 25\n",
      "finished importing chunk 26\n",
      "finished importing chunk 27\n",
      "finished importing chunk 28\n",
      "finished importing chunk 29\n",
      "finished importing chunk 30\n",
      "finished importing chunk 31\n",
      "finished importing chunk 32\n",
      "finished importing chunk 33\n",
      "finished importing chunk 34\n",
      "finished importing chunk 35\n",
      "finished importing chunk 36\n",
      "finished importing chunk 37\n",
      "finished importing chunk 38\n",
      "finished importing chunk 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,30,170,205) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 40\n",
      "finished importing chunk 41\n",
      "finished importing chunk 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,30,170,181,205,356) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 43\n",
      "finished importing chunk 44\n",
      "finished importing chunk 45\n",
      "finished importing chunk 46\n",
      "finished importing chunk 47\n",
      "finished importing chunk 48\n",
      "finished importing chunk 49\n",
      "finished importing chunk 50\n",
      "finished importing chunk 51\n",
      "finished importing chunk 52\n",
      "finished importing chunk 53\n",
      "finished importing chunk 54\n",
      "finished importing chunk 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,170,181,205,254,268,275,278,280,300,311,317,325,361) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 56\n",
      "finished importing chunk 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,170,181,205,357) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,170,181,205,254) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 59\n",
      "finished importing chunk 60\n",
      "finished importing chunk 61\n",
      "finished importing chunk 62\n",
      "finished importing chunk 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (8,205) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing chunk 64\n"
     ]
    }
   ],
   "source": [
    "#set proper working directors\n",
    "os.chdir('C:\\\\Users\\\\paperspace\\\\Desktop\\\\Microsoft\\\\AB Testing')\n",
    "\n",
    "#import data (only first n00k rows as a sample)\n",
    "chunksize = 100000\n",
    "df = pd.read_csv('end_user_nps_cy2020_only_e-exp_partial.tsv', delimiter = '\\t', chunksize = chunksize)\n",
    "\n",
    "#read in dataset that contains column names\n",
    "colnames = pd.read_excel('sample_AB.xlsx', nrows = 1)\n",
    "\n",
    "df.columns = colnames.columns\n",
    "\n",
    "#read in df by chunks\n",
    "chunks = []\n",
    "for idx, chunk in enumerate(df):\n",
    "    chunk.columns = colnames.columns\n",
    "    chunk = chunk[['XpId', 'XpGroupType', 'XpUrl', 'Id','SurveyId', 'SurveyRating', 'ProcessSessionId', 'Platform', 'Product']]\n",
    "    chunks.append(chunk)\n",
    "    print(f'finished importing chunk {idx + 1}')\n",
    "\n",
    "#concatenate chunks in list, have to pass sort=False to ignore the FutureWarning produced by this\n",
    "df = pd.concat(chunks, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send df to outfile\n",
    "df.to_csv('Stage1ABdata.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Stage1ABdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XpId</th>\n",
       "      <th>XpGroupType</th>\n",
       "      <th>XpUrl</th>\n",
       "      <th>Id</th>\n",
       "      <th>SurveyId</th>\n",
       "      <th>SurveyRating</th>\n",
       "      <th>ProcessSessionId</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38234</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=38234</td>\n",
       "      <td>flnps_v2_b7963972fdae58f698feb84fe9deb246</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1f923701-08bc-4537-9a2e-db4a17624445</td>\n",
       "      <td>Android</td>\n",
       "      <td>Android Excel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42370</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=42370</td>\n",
       "      <td>flnps_v2_b7963972fdae58f698feb84fe9deb246</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1f923701-08bc-4537-9a2e-db4a17624445</td>\n",
       "      <td>Android</td>\n",
       "      <td>Android Excel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42371</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=42371</td>\n",
       "      <td>flnps_v2_b7963972fdae58f698feb84fe9deb246</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1f923701-08bc-4537-9a2e-db4a17624445</td>\n",
       "      <td>Android</td>\n",
       "      <td>Android Excel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44777</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=44777</td>\n",
       "      <td>flnps_v2_b7963972fdae58f698feb84fe9deb246</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1f923701-08bc-4537-9a2e-db4a17624445</td>\n",
       "      <td>Android</td>\n",
       "      <td>Android Excel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38235</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=38235</td>\n",
       "      <td>flnps_v2_7c12a533885454a19ad54a76b7f80646</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>5.0</td>\n",
       "      <td>ea85437b-3cea-4a30-b590-f41a3258cf1f</td>\n",
       "      <td>Mac</td>\n",
       "      <td>Mac Powerpoint</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    XpId XpGroupType                                                XpUrl  \\\n",
       "0  38234  T           https://ecs.skype.com/?page=ExperimentPage&id=38234   \n",
       "1  42370  T           https://ecs.skype.com/?page=ExperimentPage&id=42370   \n",
       "2  42371  T           https://ecs.skype.com/?page=ExperimentPage&id=42371   \n",
       "3  44777  T           https://ecs.skype.com/?page=ExperimentPage&id=44777   \n",
       "4  38235  T           https://ecs.skype.com/?page=ExperimentPage&id=38235   \n",
       "\n",
       "                                          Id  \\\n",
       "0  flnps_v2_b7963972fdae58f698feb84fe9deb246   \n",
       "1  flnps_v2_b7963972fdae58f698feb84fe9deb246   \n",
       "2  flnps_v2_b7963972fdae58f698feb84fe9deb246   \n",
       "3  flnps_v2_b7963972fdae58f698feb84fe9deb246   \n",
       "4  flnps_v2_7c12a533885454a19ad54a76b7f80646   \n",
       "\n",
       "                               SurveyId  SurveyRating  \\\n",
       "0  092b8d95-cf3f-4172-a11b-f5bbc03077ac  1.0            \n",
       "1  092b8d95-cf3f-4172-a11b-f5bbc03077ac  1.0            \n",
       "2  092b8d95-cf3f-4172-a11b-f5bbc03077ac  1.0            \n",
       "3  092b8d95-cf3f-4172-a11b-f5bbc03077ac  1.0            \n",
       "4  092b8d95-cf3f-4172-a11b-f5bbc03077ac  5.0            \n",
       "\n",
       "                       ProcessSessionId Platform         Product  \n",
       "0  1f923701-08bc-4537-9a2e-db4a17624445  Android  Android Excel   \n",
       "1  1f923701-08bc-4537-9a2e-db4a17624445  Android  Android Excel   \n",
       "2  1f923701-08bc-4537-9a2e-db4a17624445  Android  Android Excel   \n",
       "3  1f923701-08bc-4537-9a2e-db4a17624445  Android  Android Excel   \n",
       "4  ea85437b-3cea-4a30-b590-f41a3258cf1f  Mac      Mac Powerpoint  "
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging NPS Question Types\n",
    "- I am going to be finding out whether a user was asked about the full suite of Office 365 Products, or whether they were asked about a specific app, I will then use that information to tag each survey answer, which I can then use to split the data into \"suite\" and \"app\" subsets. Once the data is filtered, I will then feed each of these chunks into the function below to generate a report for A/B testing. I am also going to be feeding in the full dataframe, not separated by O365 and application questions, to show what the impact of the experiments were on the overall data. \n",
    "- Tested this with Sensei/DM as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Google Translate to translate foreign language questions into english (this may not work on some VPNs, so if you are having trouble with this\n",
    "# then I recommend you change VPNs)\n",
    "def translate_text(text, dest_language=\"en\"):\n",
    "        # Used to translate using the googletrans library\n",
    "    import json\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        translation = translator.translate(text=text, dest=dest_language)\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        # api call restriction\n",
    "        process = subprocess.Popen([\"nordvpn\", \"d\"], stdout=subprocess.PIPE)\n",
    "        process.wait()\n",
    "        process = subprocess.Popen([\"nordvpn\", \"c\", \"canada\"], stdout=subprocess.PIPE)\n",
    "        process.wait()\n",
    "        return Process_Data.translate_text(text=text, dest_language=dest_language)\n",
    "    return translation\n",
    "\n",
    "#translate all questions\n",
    "question_list = [x for x in questions.Question]\n",
    "\n",
    "translated_questions = [translate_text(x).text for x in question_list]\n",
    "\n",
    "questions.insert(loc = 1, column = 'TranslatedQuestions', value = [question for question in translated_questions])\n",
    "\n",
    "questions.to_csv('translated_questions_from_NPS_survey.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_questions = pd.read_csv('translated_questions._from_NPS_survey.csv')\n",
    "#merging translated questions into original df, dropping redundant question column \n",
    "df = translated_questions.join(df.set_index(df.SurveyRatingQuestion), on = 'Question').drop(['Question'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for keywords in the df and separating products out by keyword -- this generates some duplicates becuase some questions include BOTH the\n",
    "# name of an app (i.e. Word) AND the word \"app\" as well, so we need to filter out duplicates with boolean logic after concatenating all of them, then filter \n",
    "# -- Therea are about 20 surveys that we will not capture filtering out in this manner that refer to \"games\" or other things that were simply lost in \n",
    "#translation and unable to be tied back to a specific product\n",
    "products = ['365|Office|office', \n",
    "            \"application|apli|App|app\", \n",
    "            'Word|word', 'Excel|excel', \n",
    "            'Powerpoint|powerpoint|PowerPoint', 'Outlook|outlook', 'Access|access', 'Visi|visi', \n",
    "            'OneNote', 'program', 'Proj|proj']\n",
    "\n",
    "d = {}\n",
    "for name in products:\n",
    "    d[name] = pd.DataFrame(df[(df['TranslatedQuestion'].astype(str).str.contains(f'{name}')) | (df['SurveyRatingQuestion'].astype(str).str.contains(f'{name}'))])\n",
    "\n",
    "#concatenating dfs together to see exactly how many survey responses we didn't capture using the above filtering\n",
    "dfs = []\n",
    "for x in d.keys():\n",
    "    prod = pd.DataFrame(d[x])\n",
    "    dfs.append(prod)\n",
    "\n",
    "    \n",
    "nps_df = pd.concat(dfs, axis = 0).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out cases that have NPS questions pertaining to the suite and speicific apps\n",
    "suite = nps_df[nps_df['TranslatedQuestion'].str.contains('365|Office|office|offi|ofi')]\n",
    "\n",
    "app = nps_df[~nps_df.index.isin(suite.index)]\n",
    "\n",
    "#add in a tag \n",
    "suite.insert(loc = 0, column = 'NPS Tag', value = 'O365 Suite')\n",
    "app.insert(loc = 0, column = 'NPS Tag', value = 'App')\n",
    "\n",
    "nps_df.to_csv('NPS_data_separated_by_suite_and_app.csv', index = False)\n",
    "\n",
    "#set proper working directors\n",
    "os.chdir('C:\\\\Users\\\\fulto\\\\Desktop\\\\AB Testing\\\\AB Test Scripts')\n",
    "suite_and_app_df = pd.read_csv('NPS_data_separated_by_suite_and_app.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XpId</th>\n",
       "      <th>XpGroupType</th>\n",
       "      <th>XpUrl</th>\n",
       "      <th>Id</th>\n",
       "      <th>SurveyId</th>\n",
       "      <th>SurveyRating</th>\n",
       "      <th>SurveyRatingQuestion</th>\n",
       "      <th>ProcessSessionId</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Product</th>\n",
       "      <th>NPS Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38234</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=38234</td>\n",
       "      <td>flnps_v2_b7963972fdae58f698feb84fe9deb246</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>How likely are you to recommend Office 365 to a friend or colleague?</td>\n",
       "      <td>1f923701-08bc-4537-9a2e-db4a17624445</td>\n",
       "      <td>Android</td>\n",
       "      <td>Android Excel</td>\n",
       "      <td>O365 Suite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42370</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=42370</td>\n",
       "      <td>flnps_v2_b7963972fdae58f698feb84fe9deb246</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>How likely are you to recommend Office 365 to a friend or colleague?</td>\n",
       "      <td>1f923701-08bc-4537-9a2e-db4a17624445</td>\n",
       "      <td>Android</td>\n",
       "      <td>Android Excel</td>\n",
       "      <td>O365 Suite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42371</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=42371</td>\n",
       "      <td>flnps_v2_b7963972fdae58f698feb84fe9deb246</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>How likely are you to recommend Office 365 to a friend or colleague?</td>\n",
       "      <td>1f923701-08bc-4537-9a2e-db4a17624445</td>\n",
       "      <td>Android</td>\n",
       "      <td>Android Excel</td>\n",
       "      <td>O365 Suite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44777</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=44777</td>\n",
       "      <td>flnps_v2_b7963972fdae58f698feb84fe9deb246</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>How likely are you to recommend Office 365 to a friend or colleague?</td>\n",
       "      <td>1f923701-08bc-4537-9a2e-db4a17624445</td>\n",
       "      <td>Android</td>\n",
       "      <td>Android Excel</td>\n",
       "      <td>O365 Suite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38235</td>\n",
       "      <td>T</td>\n",
       "      <td>https://ecs.skype.com/?page=ExperimentPage&amp;id=38235</td>\n",
       "      <td>flnps_v2_7c12a533885454a19ad54a76b7f80646</td>\n",
       "      <td>092b8d95-cf3f-4172-a11b-f5bbc03077ac</td>\n",
       "      <td>5.0</td>\n",
       "      <td>How likely are you to recommend Office 365 to a friend or colleague?</td>\n",
       "      <td>ea85437b-3cea-4a30-b590-f41a3258cf1f</td>\n",
       "      <td>Mac</td>\n",
       "      <td>Mac Powerpoint</td>\n",
       "      <td>O365 Suite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    XpId XpGroupType                                                XpUrl  \\\n",
       "0  38234  T           https://ecs.skype.com/?page=ExperimentPage&id=38234   \n",
       "1  42370  T           https://ecs.skype.com/?page=ExperimentPage&id=42370   \n",
       "2  42371  T           https://ecs.skype.com/?page=ExperimentPage&id=42371   \n",
       "3  44777  T           https://ecs.skype.com/?page=ExperimentPage&id=44777   \n",
       "4  38235  T           https://ecs.skype.com/?page=ExperimentPage&id=38235   \n",
       "\n",
       "                                          Id  \\\n",
       "0  flnps_v2_b7963972fdae58f698feb84fe9deb246   \n",
       "1  flnps_v2_b7963972fdae58f698feb84fe9deb246   \n",
       "2  flnps_v2_b7963972fdae58f698feb84fe9deb246   \n",
       "3  flnps_v2_b7963972fdae58f698feb84fe9deb246   \n",
       "4  flnps_v2_7c12a533885454a19ad54a76b7f80646   \n",
       "\n",
       "                               SurveyId  SurveyRating  \\\n",
       "0  092b8d95-cf3f-4172-a11b-f5bbc03077ac  1.0            \n",
       "1  092b8d95-cf3f-4172-a11b-f5bbc03077ac  1.0            \n",
       "2  092b8d95-cf3f-4172-a11b-f5bbc03077ac  1.0            \n",
       "3  092b8d95-cf3f-4172-a11b-f5bbc03077ac  1.0            \n",
       "4  092b8d95-cf3f-4172-a11b-f5bbc03077ac  5.0            \n",
       "\n",
       "                                                   SurveyRatingQuestion  \\\n",
       "0  How likely are you to recommend Office 365 to a friend or colleague?   \n",
       "1  How likely are you to recommend Office 365 to a friend or colleague?   \n",
       "2  How likely are you to recommend Office 365 to a friend or colleague?   \n",
       "3  How likely are you to recommend Office 365 to a friend or colleague?   \n",
       "4  How likely are you to recommend Office 365 to a friend or colleague?   \n",
       "\n",
       "                       ProcessSessionId Platform         Product     NPS Tag  \n",
       "0  1f923701-08bc-4537-9a2e-db4a17624445  Android  Android Excel   O365 Suite  \n",
       "1  1f923701-08bc-4537-9a2e-db4a17624445  Android  Android Excel   O365 Suite  \n",
       "2  1f923701-08bc-4537-9a2e-db4a17624445  Android  Android Excel   O365 Suite  \n",
       "3  1f923701-08bc-4537-9a2e-db4a17624445  Android  Android Excel   O365 Suite  \n",
       "4  ea85437b-3cea-4a30-b590-f41a3258cf1f  Mac      Mac Powerpoint  O365 Suite  "
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at the head of the data, this is what it should look like to be fed properly into the scoring function\n",
    "suite_and_app_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check on questions again to see if it looks right -- it does\n",
    "#suite_and_app_df.groupby(['NPS Tag']).SurveyRatingQuestion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate out suite and app dfs\n",
    "suite_df = suite_and_app_df[suite_and_app_df['NPS Tag'] == 'O365 Suite'].drop('NPS Tag', axis = 1)\n",
    "app_df = suite_and_app_df[suite_and_app_df['NPS Tag'] == 'App'].drop('NPS Tag', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Function To Run A/B Test\n",
    "- This function will run the entire analysis in one go\n",
    "- details are commented out for eachn line of code\n",
    "- esentially this takes the df we pulled above, calculates the Standard Error of the NPS for the treatment and control groups, calculates the Margin of Error at 95% CI, then creates upper and lower bounds (CIs) fot the treatment and control groups, then finally determines if a flight was statistically significant. \n",
    "- All of this information is fed into two final output dataframes, one for platform aggregated info and the other for product specific tests\n",
    "- Beware that there are some flights that have no treatment group and visa versa, sso I filtered the data further to ONLY contain treatment-control pairs, (experiments that contained both control AND treatment groups). From here, I also filtered the data to only contain treatment-control pairs where there were greater than 30 individuals (60 total) in each group. The resulting dataframe is a fraction of the total number of experiments, but contains no solitary groups and no underpowered samples (some of the samples only had one person in them, which also generates \"infinite\" answers, so be on the lookout for that as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---->---->Creating and Scoring Data Frame for Experiments Aggregated By Product<----<-----\n",
      " \n",
      "----->----->Scoring Control Group----->----->\n",
      " \n",
      "----->----->Scoring Treatment Group----->----->\n",
      " \n",
      "---->---->Creating and Scoring Each Experiment, Broken Down By Product, Starting With Control Group<----<----\n",
      " \n",
      "---->---->Scoring Control Groups---->---->\n",
      " \n",
      "---->---->Scoring Treatment Group---->---->\n",
      " \n",
      " \n",
      "---------------->Function Executed Successfully<---------------------\n",
      "  \n",
      "  \n",
      "*******AB Test Complete**********\n",
      "---->---->Creating and Scoring Data Frame for Experiments Aggregated By Product<----<-----\n",
      " \n",
      "----->----->Scoring Control Group----->----->\n",
      " \n",
      "----->----->Scoring Treatment Group----->----->\n",
      " \n",
      "---->---->Creating and Scoring Each Experiment, Broken Down By Product, Starting With Control Group<----<----\n",
      " \n",
      "---->---->Scoring Control Groups---->---->\n",
      " \n",
      "---->---->Scoring Treatment Group---->---->\n",
      " \n",
      " \n",
      "---------------->Function Executed Successfully<---------------------\n",
      "  \n",
      "  \n",
      "*******AB Test Complete**********\n",
      "---->---->Creating and Scoring Data Frame for Experiments Aggregated By Product<----<-----\n",
      " \n",
      "----->----->Scoring Control Group----->----->\n",
      " \n",
      "----->----->Scoring Treatment Group----->----->\n",
      " \n",
      "---->---->Creating and Scoring Each Experiment, Broken Down By Product, Starting With Control Group<----<----\n",
      " \n",
      "---->---->Scoring Control Groups---->---->\n",
      " \n",
      "---->---->Scoring Treatment Group---->---->\n",
      " \n",
      " \n",
      "---------------->Function Executed Successfully<---------------------\n",
      "  \n",
      "  \n",
      "*******AB Test Complete**********\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def run_ab_test(df):\n",
    "\n",
    "    '''\n",
    "    \n",
    "    This function takes an input df, using these columns from NPS data ('XpId', 'XpGroupType', 'XpUrl', 'Id', 'SurveyRating', 'ProcessSessionId', 'Platform', 'Product'), \n",
    "    It then takes this df and scores it for MOE and statsig, outputs two final dfs, one of them that is aggregated at the experiment level by product, and another that \n",
    "    has this information broken down for every product included in every experiment\n",
    "    \n",
    "    Arguments: one df that has the columns from NPS data specified in the sentence above this\n",
    "    \n",
    "    Calling the Function: calling this function requires tuple unpacking (i.e. df1, df2 = fun_ab_test(df))\n",
    "    \n",
    "    '''\n",
    "    print('---->---->Creating and Scoring Data Frame for Experiments Aggregated By Product<----<-----')\n",
    "    print(' ')\n",
    "    print('----->----->Scoring Control Group----->----->')\n",
    "\n",
    "    #rename columns to more intuitive names\n",
    "    df = df.rename(columns = {'XpId':'ExperimentId', 'XpUrl':'ExperimentUrl', 'XpGroupType':'ExperimentGroup', 'Id':'SurveyId', 'XpUrl':'ExperimentUrl', 'SurveyRating':'NPS'})\n",
    "\n",
    "    #map new scores to NPS --> 1-3 = -100, 4 = 0, 5 = 100\n",
    "    df['NPS'] = df['NPS'].map({1:-100, 2:-100, 3:-100, 4:0, 5:100})\n",
    "    \n",
    "    print(' ')\n",
    "\n",
    "    #creating groupby data frame and then unstacking it -- this is the primary meat of this analysis\n",
    "    score_df = pd.DataFrame(df.groupby(['ExperimentId','ExperimentGroup']).agg({'NPS':np.mean, 'ExperimentGroup':'size'}).unstack())\n",
    "\n",
    "    #rename column from multi index to single index\n",
    "    score_df.columns = ['NPS Control', 'NPS Treatment', 'Control Count', 'Treatment Count']\n",
    "\n",
    "    #fill na values with 0 to indicate that there was nobody in a specific group\n",
    "    score_df = score_df.fillna(0).reset_index()\n",
    "\n",
    "    #create column for difference in scores\n",
    "    score_df['Difference (T - C)'] = score_df['NPS Treatment'] - score_df['NPS Control']\n",
    "    #score_df.head(15)\n",
    "\n",
    "    #get # of passive, promoters, and detractors for each control group\n",
    "    control = df[df['ExperimentGroup'] == 'C'].groupby(['ExperimentId', 'NPS']).agg({'NPS':'size'}).unstack()\n",
    "\n",
    "    #rename columns\n",
    "    control.columns = ['Detractors', 'Passives', 'Promoters']\n",
    "\n",
    "    #create total N size for control groups\n",
    "    control['Total N'] = control.sum(1)\n",
    "\n",
    "    #fill na values with 0 since this indicates that there was no score present, also reset the index to make sure experiment ID \n",
    "    #is a column\n",
    "    control = control.fillna(0).reset_index()\n",
    "\n",
    "    #find out which experiments did not have control groups\n",
    "    no_control_groups = []\n",
    "    for x in list(score_df.ExperimentId):\n",
    "        if x not in list(control.ExperimentId):\n",
    "            no_control_groups.append(x)\n",
    "\n",
    "    #add those experiments to the control group df, we will then sort the df in order of experiment number \n",
    "    new_rows = []\n",
    "    for x in no_control_groups:\n",
    "        row = pd.Series([x,0,0,0,0], index=['ExperimentId', 'Detractors', 'Passives', 'Promoters', 'Total N'])\n",
    "        new_rows.append(row)\n",
    "    \n",
    "    #create new df from no control rows\n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    #concatenate the new rows (no control group) with the control group df\n",
    "    control = pd.concat([control, new_rows_df], axis = 0).sort_values(by = 'ExperimentId').reset_index(drop = True)\n",
    "\n",
    "    #sort our dfs by experiment so they are in the same order now\n",
    "    control = control.sort_values('ExperimentId')\n",
    "    score_df = score_df.sort_values('ExperimentId')\n",
    "\n",
    "    #now we can add in the control group NPS to the correct experiment (there will be no score for exps that didn't have a control group)\n",
    "    control['NPS'] = score_df['NPS Control']\n",
    "\n",
    "    #creating function for calculating MOE, taken from formula provided in NPS wiki\n",
    "    def calculate_MOE(nps, promoters, passives, detractors, N):\n",
    "        '''\n",
    "        This function calculates Margin of Error for a given treatment or control group\n",
    "        \n",
    "        Arguments: (nps = net promoter score for group, promoters|passives|detractors = N size for each subset of a group, N = total N size for the group overall)\n",
    "        \n",
    "        '''\n",
    "        a = (1 - nps)**2\n",
    "        b = promoters/N\n",
    "        c = (0 - nps)**2\n",
    "        d = passives/N\n",
    "        e = (-1 - nps)**2\n",
    "        f = detractors/N\n",
    "        g = N - 1\n",
    "        moe = 1.96 * np.sqrt((a * b + c * d + e * f) / (g))\n",
    "        return moe\n",
    "\n",
    "    #calculating MOE for control group\n",
    "    control['MOE Control'] = calculate_MOE(control['NPS'], control['Promoters'], control['Passives'], control['Detractors'], control['Total N'])\n",
    "\n",
    "    #filling in NA values (where there was no control group, so not MOE)\n",
    "    control.fillna(0, inplace = True)\n",
    "\n",
    "    #creating column in score_df for the Control MOE to be used in lower and upper confidence interval calculations\n",
    "    score_df['MOE Control'] = control['MOE Control']\n",
    "\n",
    "    #creating upper and lower bound calculations for the Control group and adding them as columns to the score df\n",
    "    score_df['Control Lower Bound'] = score_df['NPS Control'] - score_df['MOE Control']\n",
    "    score_df['Control Upper Bound'] = score_df['NPS Control'] + score_df['MOE Control']\n",
    "\n",
    "    print('----->----->Scoring Treatment Group----->----->')\n",
    "    # #creating treatment df now\n",
    "    # #get # of passive, promoters, and detractors for each control group\n",
    "    treatment = df[df['ExperimentGroup'] == 'T'].groupby(['ExperimentId', 'NPS']).agg({'NPS':'size'}).unstack()\n",
    "\n",
    "\n",
    "    ##rename columns\n",
    "    treatment.columns = ['Detractors', 'Passives', 'Promoters']\n",
    "\n",
    "\n",
    "    #create total N size for control groups\n",
    "    treatment['Total N'] = treatment.sum(1)\n",
    "\n",
    "\n",
    "    #fill na values with 0 since this indicates that there was no score present, also reset the index to make sure experiment ID \n",
    "    #is a column\n",
    "    treatment = treatment.fillna(0).reset_index()\n",
    "\n",
    "    #add in NPS score for control group as a column\n",
    "    #treatment['NPS Treatment'] = score_df['NPS Treatment']\n",
    "\n",
    "\n",
    "    #find out which experiments did not have treatment groups\n",
    "    no_treatment_groups = []\n",
    "    for x in list(score_df.ExperimentId):\n",
    "        if x not in list(treatment.ExperimentId):\n",
    "            no_treatment_groups.append(x)\n",
    "\n",
    "    #add those experiments to the control group df, we will then sort the df in order of experiment number \n",
    "    new_treatment_rows = []\n",
    "    for x in no_treatment_groups:\n",
    "        row = pd.Series([x,0,0,0,0], index=['ExperimentId', 'Detractors', 'Passives', 'Promoters', 'Total N'])\n",
    "        new_treatment_rows.append(row)\n",
    "    \n",
    "    #create new df from no control rows\n",
    "    new_treatment_rows_df = pd.DataFrame(new_treatment_rows)\n",
    "\n",
    "    #concatenate the new rows (no control group) with the control group df\n",
    "    treatment = pd.concat([treatment, new_treatment_rows_df], axis = 0, sort = False).sort_values(by = 'ExperimentId').reset_index(drop = True)\n",
    "\n",
    "    #sort our dfs by experiment so they are in the same order now\n",
    "    treatment = treatment.sort_values('ExperimentId')\n",
    "\n",
    "\n",
    "    #now we can add in the treatment group NPS to the correct experiment (there will be no score for exps that didn't have a treatment group)\n",
    "    treatment['NPS Treatment'] = score_df['NPS Treatment']\n",
    "\n",
    "\n",
    "    #calculating MOE for treatment group using function created eariler\n",
    "    treatment['MOE Treatment'] = calculate_MOE(treatment['NPS Treatment'], treatment['Promoters'], treatment['Passives'], treatment['Detractors'], \n",
    "                                         treatment['Total N'])\n",
    "\n",
    "    #filling NA values (where there was no treatment group) -- only 5 or 6 that had low N and only control\n",
    "    treatment.fillna(0, inplace = True)\n",
    "\n",
    "    #creating column in score_df for the Control MOE to be used in lower and upper confidence interval calculations\n",
    "    score_df['MOE Treatment'] = treatment['MOE Treatment']\n",
    "\n",
    "    #creating upper and lower bound calculations for the Control group and adding them as columns to the score df\n",
    "    score_df['Treatment Lower Bound'] = score_df['NPS Treatment'] - score_df['MOE Treatment']\n",
    "    score_df['Treatment Upper Bound'] = score_df['NPS Treatment'] + score_df['MOE Treatment']\n",
    "    \n",
    "    #create boolean lists for if the lower and upper bounds of the control were greater/less than those of the treatment group\n",
    "    a = (score_df['Control Upper Bound'] > score_df['Treatment Lower Bound']) & (score_df['Control Lower Bound'] > score_df['Treatment Lower Bound'])\n",
    "    b = (score_df['Control Upper Bound'] > score_df['Treatment Upper Bound']) & (score_df['Control Lower Bound'] > score_df['Treatment Upper Bound'])\n",
    "    c = (score_df['Treatment Upper Bound'] > score_df['Control Lower Bound']) & (score_df['Treatment Lower Bound'] > score_df['Control Lower Bound'])\n",
    "    d = (score_df['Treatment Upper Bound'] > score_df['Control Upper Bound']) & (score_df['Treatment Lower Bound'] > score_df['Control Upper Bound'])\n",
    "    \n",
    "    #concatenate control and treatment pairs\n",
    "    control_statsig = pd.concat([a, b], axis = 1)\n",
    "    treatment_statsig = pd.concat([c, d], axis = 1)\n",
    "    \n",
    "    #must have a pair of True values (either two on left, or two on right) to be statsig\n",
    "    control_statsig = pd.DataFrame(control_statsig.sum(1) == 2).rename(columns = {0:'StatSig?'})\n",
    "    control_sig_indices = control_statsig[control_statsig['StatSig?'] == True].index\n",
    "    \n",
    "    #must have a pair of True values (either two on left, or two on right) to be statsig\n",
    "    treatment_statsig = pd.DataFrame(treatment_statsig.sum(1) == 2).rename(columns = {0:'StatSig?'})\n",
    "    treatment_sig_indices = treatment_statsig[treatment_statsig['StatSig?'] == True].index\n",
    "    \n",
    "    #create series so that you can concatenate them next into one column then one list\n",
    "    treatment_sig_indices = pd.Series([x for x in treatment_sig_indices])\n",
    "    control_sig_indices = pd.Series([x for x in control_sig_indices])\n",
    "    \n",
    "    #get statistically significant index numbers to filter out statsig experiments\n",
    "    sig_indices = [x for x in pd.concat([treatment_sig_indices, control_sig_indices], axis = 0).reset_index(drop = True)]\n",
    "    \n",
    "    #run loop to tag StatSig experiments and create a column for their labels\n",
    "    statsig = []\n",
    "    for x in score_df.index:\n",
    "        if x in sig_indices:\n",
    "            statsig.append('StatSig')\n",
    "        else:\n",
    "            statsig.append('Not')\n",
    "    score_df['StatSig'] = statsig\n",
    "    \n",
    "    #get product and platform for experiments and add them in as columns\n",
    "    unique_xps = df.drop_duplicates(subset = 'ExperimentId')[['ExperimentId', 'Platform', 'Product']].sort_values('ExperimentId').reset_index(drop = True)\n",
    "    \n",
    "    #insert platform and product columns\n",
    "    score_df.insert(loc=1, column = 'Platform', value = unique_xps['Platform'])\n",
    "    \n",
    "    def get_products_in_experiment(df):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        This function takes the same df as an input and inserts a column that includes all of the products that an experiment was conducted in\n",
    "        \n",
    "        '''\n",
    "        #group experimnets by thei id and the product\n",
    "        experiment_products = df.groupby('ExperimentId').Product.value_counts().unstack().reset_index()\n",
    "        \n",
    "        #create a dataframe that we can use to tag values that are non-NaN with the name of the product\n",
    "        a = pd.DataFrame(experiment_products.iloc[:, 1]).fillna(0)\n",
    "        new = []\n",
    "        for x in [x for x in a.iloc[:, 0]]:\n",
    "            if x != 0:\n",
    "                new.append(\", \".join(a.columns.tolist()))\n",
    "            else:\n",
    "                new.append(np.nan)\n",
    "        a[a.columns] = new\n",
    "        \n",
    "        #create lest to store names of products in their row\n",
    "        new = []\n",
    "        for col in experiment_products.columns:\n",
    "            a = pd.DataFrame(experiment_products.loc[:, f'{col}']).fillna(0)\n",
    "            for x in [x for x in a.iloc[:, 0]]:\n",
    "                if x != 0:\n",
    "                    new.append(', '.join(a.columns.tolist()))\n",
    "                else:\n",
    "                    new.append(np.nan)\n",
    "                \n",
    "        # Create a function called \"chunks\" with two arguments, l and n:\n",
    "        def chunks(l, n):\n",
    "            # For item i in a range that is a length of l,\n",
    "            for i in range(0, len(l), n):\n",
    "                # Create an index range for l of n items:\n",
    "                yield l[i:i+n]\n",
    "        \n",
    "        #create df that has values replaces with product names, and has 0s otherwise\n",
    "        new_exp_prod = pd.DataFrame([x for x in chunks(new, experiment_products.shape[0])]).T\n",
    "        new_exp_prod.columns = experiment_products.columns\n",
    "        new_exp_prod['ExperimentId'] = experiment_products['ExperimentId']\n",
    "        new_exp_prod.index = new_exp_prod['ExperimentId']\n",
    "        \n",
    "        #create list to store product names\n",
    "        prod_names = []\n",
    "        for idx, row in new_exp_prod.fillna(0).iterrows():\n",
    "            for x in row[1:]:\n",
    "                if (x != 0) & (x != 'ExperimentId'):\n",
    "                    prod_names.append(x)\n",
    "                else:\n",
    "                    prod_names.append(0)\n",
    "        #create final list to store product names and throw them into the df\n",
    "        final_list = []\n",
    "        for idx, row in new_exp_prod.iterrows():\n",
    "            final_list.append([x for x in row[1:] if str(x) != 'nan'])\n",
    "        \n",
    "        return pd.DataFrame({0:[x for x in final_list]})\n",
    "    \n",
    "    #insert these as a column, this is the column that contains all product names for a given experiment ID\n",
    "    score_df.insert(loc = 1, column = 'Products', value = [x for x in get_products_in_experiment(df).iloc[:, 0]])\n",
    "    \n",
    "    #this will get rid of brackets for the column values\n",
    "    score_df['Products'] = [\", \".join(x) for x in score_df['Products']]\n",
    "    \n",
    "    #add in the Experiment URL\n",
    "    score_df.insert(loc = 1, column = 'ExperimentUrl', value = pd.DataFrame(df.sort_values('ExperimentId').ExperimentUrl.unique()))\n",
    "    \n",
    "    #filter out experiments that had sample sizes of less than 30 in EITHER the treatment or control groups or if either one of the groups was not present\n",
    "    score_df = score_df[(score_df['Control Count'] > 30) & (score_df['Treatment Count'] > 30) & ((score_df['Control Count'] != 0) & score_df['Treatment Count'] != 0)]\n",
    "    \n",
    "    #reset the index\n",
    "    score_df = score_df.reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Now the function will create a separate scored data frame for each experiment, but broken down by product\n",
    "    print(' ')\n",
    "    print('---->---->Creating and Scoring Each Experiment, Broken Down By Product, Starting With Control Group<----<----')\n",
    "    print(' ')\n",
    "    print('---->---->Scoring Control Groups---->---->')\n",
    "    print(' ')\n",
    "    \n",
    "    #rename columns to more intuitive names\n",
    "    df = df.rename(columns = {'XpId':'ExperimentId', 'XpUrl':'ExperimentUrl', 'XpGroupType':'ExperimentGroup', 'Id':'SurveyId', 'XpUrl':'ExperimentUrl', 'SurveyRating':'NPS'})\n",
    "\n",
    "    #map new scores to NPS --> 1-3 = -100, 4 = 0, 5 = 100\n",
    "    #df['NPS'] = df['NPS'].map({1:-100, 2:-100, 3:-100, 4:0, 5:100})\n",
    "\n",
    "    all_prods = pd.DataFrame(df.groupby(['ExperimentId','Platform', 'Product', 'ExperimentGroup']).agg({'NPS':np.mean, 'ExperimentGroup':'size'}).unstack()).reset_index()\n",
    "\n",
    "    all_prods.fillna(0, inplace = True)\n",
    "\n",
    "    #rename columns\n",
    "    all_prods.columns = ['ExperimentId', 'Platform', 'Product', 'NPS Control', 'NPS Treatment', 'ControlSize', 'TreatmentSize']\n",
    "\n",
    "    all_prods = all_prods[(all_prods['ControlSize'] > 30) & (all_prods['TreatmentSize'] > 30)]\n",
    "\n",
    "    all_prods['Difference (T - C)'] = all_prods['NPS Treatment'] - all_prods['NPS Control']\n",
    "\n",
    "    #filter out for control groups\n",
    "    all_prods_control = df[df['ExperimentGroup'] == 'C'].groupby(['ExperimentId','Product', 'NPS']).agg({'NPS':'size'}).unstack()\n",
    "\n",
    "    all_prods_control.fillna(0, inplace = True)\n",
    "\n",
    "    #rename columns\n",
    "    all_prods_control.columns = ['Detractors', 'Passives', 'Promoters']\n",
    "\n",
    "    #create total N size for control groups\n",
    "    all_prods_control['Total N'] = all_prods_control.sum(1)\n",
    "\n",
    "    all_prods_control = all_prods_control.fillna(0).reset_index()\n",
    "\n",
    "    all_prods_control = all_prods_control[(all_prods_control['Total N'] > 30)]\n",
    "\n",
    "    no_control_groups = []\n",
    "    for x in list(all_prods.ExperimentId):\n",
    "        if x not in list(all_prods_control.ExperimentId):\n",
    "            no_control_groups.append(x)\n",
    "\n",
    "    new_rows = []\n",
    "    for x in no_control_groups:\n",
    "        row = pd.Series([x,0,0,0,0,0], index=['ExperimentId', 'Product', 'Detractors', 'Passives', 'Promoters', 'Total N'])\n",
    "        new_rows.append(row)\n",
    "\n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    #concatenate the new rows (no control group) with the control group df\n",
    "    all_prods_control = pd.concat([all_prods_control, new_rows_df], axis = 0).sort_values(by = 'ExperimentId').reset_index(drop = True)\n",
    "\n",
    "    all_prods_control = all_prods_control[all_prods_control['Product'] != 0].reset_index(drop = True)\n",
    "\n",
    "    #sort our dfs by experiment so they are in the same order now\n",
    "    all_prods_control = all_prods_control.sort_values('ExperimentId')\n",
    "    all_prods = all_prods.sort_values('ExperimentId')\n",
    "\n",
    "    drop_indices = []\n",
    "    for x, y in enumerate(zip(all_prods_control['ExperimentId'], all_prods_control['Product'])):\n",
    "        if y not in zip(all_prods['ExperimentId'], all_prods['Product']):\n",
    "            drop_indices.append(x)\n",
    "    all_prods_control = all_prods_control.drop(all_prods_control.index[drop_indices]).sort_values('ExperimentId')\n",
    "\n",
    "    #sort values by BOTH ID and Product\n",
    "    all_prods = all_prods.sort_values(by = ['ExperimentId', 'Product'])\n",
    "    all_prods_control = all_prods_control.sort_values(by = ['ExperimentId', 'Product'])\n",
    "    \n",
    "    #now we can add in the control group NPS to the correct experiment (there will be no score for exps that didn't have a control group)\n",
    "    all_prods_control['NPS Control'] = [x for x in all_prods['NPS Control']]\n",
    "\n",
    "    #calculating MOE for control group\n",
    "    all_prods_control['MOE Control'] = calculate_MOE(all_prods_control['NPS Control'], all_prods_control['Promoters'], all_prods_control['Passives'], all_prods_control['Detractors'], \n",
    "                                                 all_prods_control['Total N'])\n",
    "\n",
    "    all_prods_control.fillna(0, inplace = True)\n",
    "\n",
    "    all_prods['MOE Control'] = [x for x in all_prods_control['MOE Control']]\n",
    "\n",
    "    #creating upper and lower bound calculations for the Control group and adding them as columns to the score df\n",
    "    all_prods_control['Control Lower Bound'] = all_prods_control['NPS Control'] - all_prods_control['MOE Control']\n",
    "    all_prods_control['Control Upper Bound'] = all_prods_control['NPS Control'] + all_prods_control['MOE Control']\n",
    "\n",
    "    all_prods_control = all_prods_control[all_prods_control['Total N'] > 30].reset_index(drop = True)\n",
    "    \n",
    "    print('---->---->Scoring Treatment Group---->---->')\n",
    "    print(' ')\n",
    "\n",
    "    #filter out for control groups\n",
    "    all_prods_treatment = df[df['ExperimentGroup'] == 'T'].groupby(['ExperimentId','Product', 'NPS']).agg({'NPS':'size'}).unstack()\n",
    "\n",
    "    all_prods_treatment.fillna(0, inplace = True)\n",
    "\n",
    "    #rename columns\n",
    "    all_prods_treatment.columns = ['Detractors', 'Passives', 'Promoters']\n",
    "\n",
    "    #create total N size for control groups\n",
    "    all_prods_treatment['Total Treatment N'] = all_prods_treatment.sum(1)\n",
    "\n",
    "    all_prods_treatment = all_prods_treatment.fillna(0).reset_index()\n",
    "\n",
    "    no_tx_groups = []\n",
    "    for x in list(all_prods.ExperimentId):\n",
    "        if x not in list(all_prods_treatment.ExperimentId):\n",
    "            no_tx_groups.append(x)\n",
    "\n",
    "    new_rows = []\n",
    "    for x in no_tx_groups:\n",
    "        row = pd.Series([x,0,0,0,0,0], index=['ExperimentId', 'Product', 'Detractors', 'Passives', 'Promoters', 'Total Treatment N'])\n",
    "        new_rows.append(row)\n",
    "\n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    #concatenate the new rows (no control group) with the control group df\n",
    "    all_prods_treatment = pd.concat([all_prods_treatment, new_rows_df], axis = 0).sort_values(by = 'ExperimentId').reset_index(drop = True)\n",
    "\n",
    "    all_prods_treatment = all_prods_treatment[all_prods_treatment['Product'] != 0].reset_index(drop = True)\n",
    "\n",
    "    #sort our dfs by experiment so they are in the same order now\n",
    "    all_prods_treatment = all_prods_treatment.sort_values('ExperimentId')\n",
    "    \n",
    "\n",
    "    drop_indices = []\n",
    "    for x, y in enumerate(zip(all_prods_treatment['ExperimentId'], all_prods_treatment['Product'])):\n",
    "        if y not in zip(all_prods_control['ExperimentId'], all_prods_control['Product']):\n",
    "            drop_indices.append(x)\n",
    "    all_prods_treatment = all_prods_treatment.drop(all_prods_treatment.index[drop_indices]).sort_values('ExperimentId')\n",
    "\n",
    "    #sort values by BOTH ID and Product\n",
    "    all_prods = all_prods.sort_values(by = ['ExperimentId', 'Product'])\n",
    "    all_prods_treatment = all_prods_treatment.sort_values(by = ['ExperimentId', 'Product'])\n",
    "\n",
    "    #now we can add in the control group NPS to the correct experiment (there will be no score for exps that didn't have a control group)\n",
    "    all_prods_treatment['NPS Treatment'] = [x for x in all_prods['NPS Treatment']]\n",
    "\n",
    "    #calculating MOE for control group\n",
    "    all_prods_treatment['MOE Treatment'] = calculate_MOE(all_prods_treatment['NPS Treatment'], all_prods_treatment['Promoters'], all_prods_treatment['Passives'], all_prods_treatment['Detractors'], \n",
    "                                                 all_prods_treatment['Total Treatment N'])\n",
    "\n",
    "    all_prods_treatment.fillna(0, inplace = True)\n",
    "\n",
    "    all_prods['MOE Treatment'] = [x for x in all_prods_treatment['MOE Treatment']]\n",
    "\n",
    "    #creating upper and lower bound calculations for the Control group and adding them as columns to the score df\n",
    "    all_prods_treatment['Treatment Lower Bound'] = all_prods_treatment['NPS Treatment'] - all_prods_treatment['MOE Treatment']\n",
    "    all_prods_treatment['Treatment Upper Bound'] = all_prods_treatment['NPS Treatment'] + all_prods_treatment['MOE Treatment']\n",
    "\n",
    "    all_prods_treatment = all_prods_treatment[all_prods_treatment['Total Treatment N'] > 30].reset_index(drop = True)\n",
    "    \n",
    "    all_prods_treatment.sort_values(by = ['ExperimentId', 'Product'], inplace = True)\n",
    "\n",
    "    all_prods_control.sort_values(by = ['ExperimentId', 'Product'], inplace = True)\n",
    "\n",
    "    all_prods['Control Lower Bound'] = [x for x in all_prods_control['Control Lower Bound']]\n",
    "    all_prods['Control Upper Bound'] = [x for x in all_prods_control['Control Upper Bound']]\n",
    "\n",
    "    all_prods['Treatment Lower Bound'] = [x for x in all_prods_treatment['Treatment Lower Bound']]\n",
    "    all_prods['Treatment Upper Bound'] = [x for x in all_prods_treatment['Treatment Upper Bound']]\n",
    "    \n",
    "    #determine statsig\n",
    "     #create boolean lists for if the lower and upper bounds of the control were greater/less than those of the treatment group\n",
    "    a = (all_prods['Control Upper Bound'] > all_prods['Treatment Lower Bound']) & (all_prods['Control Lower Bound'] > all_prods['Treatment Lower Bound'])\n",
    "    b = (all_prods['Control Upper Bound'] > all_prods['Treatment Upper Bound']) & (all_prods['Control Lower Bound'] > all_prods['Treatment Upper Bound'])\n",
    "    c = (all_prods['Treatment Upper Bound'] > all_prods['Control Lower Bound']) & (all_prods['Treatment Lower Bound'] > all_prods['Control Lower Bound'])\n",
    "    d = (all_prods['Treatment Upper Bound'] > all_prods['Control Upper Bound']) & (all_prods['Treatment Lower Bound'] > all_prods['Control Upper Bound'])\n",
    "    \n",
    "    #concatenate control and treatment pairs\n",
    "    control_statsig = pd.concat([a, b], axis = 1)\n",
    "    treatment_statsig = pd.concat([c, d], axis = 1)\n",
    "    \n",
    "    #must have a pair of True values (either two on left, or two on right) to be statsig\n",
    "    control_statsig = pd.DataFrame(control_statsig.sum(1) == 2).rename(columns = {0:'StatSig?'})\n",
    "    control_sig_indices = control_statsig[control_statsig['StatSig?'] == True].index\n",
    "    \n",
    "    #must have a pair of True values (either two on left, or two on right) to be statsig\n",
    "    treatment_statsig = pd.DataFrame(treatment_statsig.sum(1) == 2).rename(columns = {0:'StatSig?'})\n",
    "    treatment_sig_indices = treatment_statsig[treatment_statsig['StatSig?'] == True].index\n",
    "    \n",
    "    #create series so that you can concatenate them next into one column then one list\n",
    "    treatment_sig_indices = pd.Series([x for x in treatment_sig_indices])\n",
    "    control_sig_indices = pd.Series([x for x in control_sig_indices])\n",
    "    \n",
    "    #get statistically significant index numbers to filter out statsig experiments\n",
    "    sig_indices = [x for x in pd.concat([treatment_sig_indices, control_sig_indices], axis = 0).reset_index(drop = True)]\n",
    "    \n",
    "    #run loop to tag StatSig experiments and create a column for their labels\n",
    "    statsig = []\n",
    "    for x in all_prods.index:\n",
    "        if x in sig_indices:\n",
    "            statsig.append('StatSig')\n",
    "        else:\n",
    "            statsig.append('Not')\n",
    "    \n",
    "    #create column for statsig in the results\n",
    "    all_prods['StatSig'] = statsig\n",
    "    \n",
    "    #merge in Experiment URL from origininal df\n",
    "    all_prods = df[['ExperimentId', 'Platform', 'Product', 'ExperimentUrl']].drop_duplicates().merge(all_prods, on = ['ExperimentId', 'Platform', 'Product']) \\\n",
    "        .sort_values('ExperimentId').reset_index(drop = True)\n",
    "    \n",
    "    #select only the columns we want to send to a csvand include in report\n",
    "    all_prods = all_prods[['ExperimentId', 'Platform', 'Product', 'ExperimentUrl', 'TreatmentSize', 'ControlSize', 'NPS Treatment', 'NPS Control', 'Difference (T - C)',\n",
    "                      'MOE Treatment', 'MOE Control', 'Control Lower Bound', 'Control Upper Bound', 'Treatment Lower Bound', 'Treatment Upper Bound', 'StatSig']]\n",
    "    \n",
    "    print(' ')\n",
    "    print('---------------->Function Executed Successfully<---------------------')\n",
    "    print('  ')\n",
    "    print('  ')\n",
    "    print('*******AB Test Complete**********')\n",
    "    \n",
    "    return score_df, all_prods\n",
    "\n",
    "#call the function to run tests on the suite and app data as well as the entire df overall\n",
    "suite_platform_df, suite_product_df = run_ab_test(suite_df)\n",
    "app_platform_df, app_product_df = run_ab_test(app_df)\n",
    "both_platform_df, both_app_df = run_ab_test(suite_and_app_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1542,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send combined df to csv\n",
    "both_platform_df.to_csv('aggregated_platform_ab_test.csv', index = False)\n",
    "both_app_df.to_csv('aggregated_product_ab_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1543,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send app and suite data to csv files\n",
    "app_platform_df.to_csv('app_platform_ab_test.csv', index = False)\n",
    "app_product_df.to_csv('app_product_ab_test.csv', index = False)\n",
    "suite_platform_df.to_csv('suite_platform_ab_test.csv', index = False)\n",
    "suite_product_df.to_csv('suite_product_ab_test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
